\newif\ifarxiv
\arxivfalse

% acmart now uses unicode-math, and mathtools needs to be loaded before it
\RequirePackage{mathtools}
\documentclass[acmsmall,screen,review,anonymous,nonacm]{acmart}
\settopmatter{printacmref=false, printccs=false, printfolios=true}

\ifarxiv
\pdfoutput=1
\nolinenumbers
\usepackage[T1]{fontenc}
\usepackage[scale=0.92]{inconsolata}
\else
\usepackage{fontspec}
\setmonofont[Scale=MatchLowercase]{inconsolata}
\fi

\usepackage[supertabular]{ottalt}
\let\newlist\relax
\let\renewlist\relax
\usepackage[capitalize,nameinlink,noabbrev]{cleveref}
\usepackage{enumitem,booktabs,xspace,doi}
\usepackage{mathpartir,stmaryrd,colonequals}
\usepackage[bottom,flushmargin,multiple,para]{footmisc} % para spacing is weird and ugly

\newcommand{\repo}{https://github.com/ionathanch/CBPV/tree/join}
\newcommand{\lang}{CBPV$_{\mathrm{j}}$\@\xspace}
\newcommand{\titlebreak}{\texorpdfstring{\\}{}}
\newcommand{\TODO}{\textcolor{red}{\textbf{\textsf{TODO}}}\@\xspace}
\newcommand{\ie}{\textit{i.e.}\@\xspace}
\newcommand{\eg}{\textit{e.g.}\@\xspace}
\newcommand{\etal}{\textit{et al.}\@\xspace}
\newcommand{\opcit}{\textit{op. cit.}\@\xspace}
\newcommand{\vs}{\textit{vs.}\@\xspace}
\newcommand{\ala}{\textit{\`a la}\@\xspace}
\newcommand{\apriori}{\textit{a priori}\@\xspace}
\newcommand{\aposteriori}{\textit{a posteriori}\@\xspace}
\newcommand{\fstar}{F$^\star$\@\xspace}
\newcommand{\welltyped}{well-\hspace{0pt}typed\@\xspace}
\newcommand{\wellfounded}{well-\hspace{0pt}founded\@\xspace}
\newcommand{\wellfoundedness}{well-\hspace{0pt}foundedness\@\xspace}
\newcommand{\wellformedness}{well-\hspace{0pt}formedness\@\xspace}
\newcommand{\welldefinedness}{well-\hspace{0pt}definedness\@\xspace}
\newcommand{\crude}{crude-\hspace{0pt}but-\hspace{0pt}effective\@\xspace}

\newcommand{\thmref}[2]{%
  $\langle$\textnormal{\texttt{\href{\repo/tree/main/src/#1}{#1}:#2}}$\rangle$%
}

\newtheorem{fail}{Falsehood}[section]

\setlength{\fboxsep}{1.5pt}
\setlength{\abovecaptionskip}{0\baselineskip}
\setlength{\textfloatsep}{\baselineskip}
\setlength{\intextsep}{0.25\baselineskip}
\setlength{\jot}{0\baselineskip}

\setlist[itemize]{leftmargin=2\parindent}
\setlist[enumerate]{topsep=0pt}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}

\mathtoolsset{showmanualtags}
\newtagform{brack}{[}{]}
\urlstyle{tt}

\crefformat{enumi}{#2step~#1#3}
\Crefformat{enumi}{#2step~#1#3}
\crefrangeformat{enumi}{step~#3#1#4 to #5#2#6}
\Crefrangeformat{enumi}{step~#3#1#4 to #5#2#6}
\Crefformat{equation}{#2Equation~#1#3}

\citestyle{acmauthoryear}
\bibliographystyle{ACM-Reference-Format}

\title[]{Commuting Conversions and Join Points \titlebreak for Call-By-Push-Value}

\author{Jonathan Chan}
\orcid{0000-0003-0830-3180}
\affiliation{University of Pennsylvania}
\email{jcxz@seas.upenn.edu}

\author{Madi Gudin}
\affiliation{Amherst College}
\email{mgudin27@amherst.edu}

\author{Annabel Levy}
\affiliation{University of Maryland, Baltimore County}
\email{alevy2@umbc.edu}

\author{Stephanie Weirich}
\orcid{0000-0002-6756-9168}
\affiliation{University of Pennsylvania}
\email{sweirich@seas.upenn.edu}

\acmJournal{PACMPL}
\acmConference[short]{name}{date}{venue}
\acmVolume{}
\acmNumber{}
\acmArticle{}
\acmYear{}
\acmMonth{}
\acmISBN{}
\acmDOI{}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
       <concept_desc>Software and its engineering~Functional languages</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024.10011027</concept_id>
       <concept_desc>Software and its engineering~Control structures</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Functional languages}
\ccsdesc[500]{Software and its engineering~Control structures}

\inputott{rules}

\begin{document}

\setlength{\abovedisplayskip}{0.25\baselineskip}
\setlength{\belowdisplayskip}{0.25\baselineskip}

\begin{abstract}

\end{abstract}

\maketitle

\section{Introduction}

\TODO: introduction

Call-by-push-value (CBPV) \citep{CBPV} is a language with a strong distinction
between \emph{values} and \emph{computations} in its syntax and types.
The benefit of the distinction is that its constructs precisely describe
when computations are executed and when they are suspended as values.
With these constructs, CBPV subsumes both call-by-name (CBN) and call-by-value (CBV)
evaluation strategies of the lambda calculus,
in the sense that it represents lambda calculus terms with intended CBN or CBV semantics
as terms with different evaluation behaviour.

By separating out values from computations,
CBPV is a suitable setting for reasoning about program equivalences where evaluation order matters.
\Citet{GTT}, for instance, use $\eta$-equivalences that only hold in CBN or in CBV languages
to reason about casts in gradual typing,
and do so in a CBPV setting since it subsumes those equivalences.
In turn, $\eta$-equivalences justify compiler optimizations
such as dead branch elimination and constant subexpression elimination/constant folding,
and \citet{equational} use CBPV to verify these kinds of optimizations.

Furthermore, the syntactically explicit evaluation order of CBPV makes it low-level enough
to be suitable as a bona fide compiler intermediate representation (IR),
and not merely for modelling compiler optimizations.
In particular, \citet{CFG} show an equivalence between CBPV
and control flow graph representations (CFG) in single static assignment (SSA) form,
and thus the viability of CBPV as an IR for imperative languages in lieu of SSA-form CFGs.
In the functional setting, the clear boundary between computations and values
mediated by thunked computations as values
makes it clear where closures ought to be created during compilation,
so \citet{closures} augment CBPV with explicit closures
and reason about optimizations related to closure conversion.

\TODO: Sentence to connect above references to why we care about looking at more CBPV compiler transformations.
In this paper, we focus on a set of \emph{commuting conversion} compiler transformations,
which are source-to-source transformations in CBPV
that rearrange the syntax of programs without affecting evaluation order.
Such transformations are often coupled with others that bind intermediate computations,
such as the A-reductions that characterize A-normal form (ANF) \citep{ANF}.
However, CBPV already binds intermediate computations,
so we can isolate just the commuting conversions.
The benefit of commuting conversions is twofold:
as a compiler optimization, they expose opportunities for program inlining,
and as a compiler pass, they unnest computations to more resemble the sequential nature of low-level code.

An issue with na\"ively applying commuting conversions,
in particular in the presence of branching computations,
is that code may be duplicated;
applying many commuting conversions can then cause exponential code bloat.
To resolve this issue, we adapt the join point constructs by \citep{join}
from the lambda calculus to CBPV and adapt the type system to accommodate them.
Introducing specialized constructs rather than using existing ones
allows us to restrict their usage to only where they are needed,
which can lead to more efficient compilation later on.

This work presents a normal form for CBPV with respect to commuting conversions,
and a single-pass transformation into this normal form using join points.
We prove that this transformation is type-preserving and evaluation-preserving:
it does not change the meaning of CBPV programs.
These proofs are mechanized in Lean 4 \citep{lean}, and the proof development is provided in the supplementary materials.
In the next section, we give an overview of the language, the transformation, and its motivation,
followed by the technical contributions in the sections that follow.
We discuss related work in \cref{sec:related},
and conclude with future work in \cref{sec:future}.

\section{Overview} \label{sec:overview}

The core ideas of this paper begin with the thesis that CBPV \citep{CBPV}
is suitable as a compiler IR because it represents control flow explicitly.
In particular, it subsumes both CBN and CBV semantics of the lambda calculus:
compiling a lambda calculus term with the CBN or CBV compilation strategy yields different CBPV terms
whose execution mirrors that of the original evaluation strategy.

\begin{figure}[h]
  \begin{align}
    [[v]], [[w]] &\dblcolon= [[x]] \mid [[()]] \mid [[inl v]] \mid [[inr v]] \mid [[{m}]] \tag{values} \\
    [[m]], [[n]] &\dblcolon= [[v!]] \mid [[λx. m]] \mid [[n v]] \mid [[⟨m, m⟩]] \mid [[fst n]] \mid [[snd n]] \mid [[return v]] \tag{computations} \\
    &\mid [[let x ← n in m]] \mid [[case v of inl x ⇒ m ; inr y ⇒ m]] \nonumber
  \end{align}
  \caption{Syntax of call-by-push-value values and computations}
  \Description[]{}
  \label{fig:syntax}
\end{figure}

CBPV syntactically distinguishes values and computations, listed in \cref{fig:syntax},
using explicit thunks to turn suspended computations into values
and explicit returns to embed values into computations,
which the CBN and CBV translations use in different ways to enforce when computation occurs.
Alongside the thunk and return constructs,
we include the unit value, value sums, functions, and computation pairs.%
\footnote{For simplicity, we omit value pairs, which are present in Levy's original CBPV;
they are interesting in our setting because the eliminator is pattern matching,
but we already have pattern matching on value sums to consider.}

As an example of the way CBN and CBV translations differ,
consider the lambda calculus term $[[(λx. x) ((λy. y) z)]]$,
which translates to the following two CBPV terms.
%
\begin{align*}
  \textbf{CBN} &\quad [[:concrete: (λx. x!) {(λy. y!) {z!}}]] \\
  \textbf{CBV} &\quad [[:concrete: let f ← return {λx. return x} in let a ← (let g ← return {λy. return y} in g! z) in f! a]]
\end{align*}

While both terms evaluate to the same final value $[[z]]$,
their evaluation sequences are different.
In the CBN translation, function arguments are thunked and passed wholesale,
then forced as they are needed.
In the CBV translation, the function and the argument are evaluated in order
before carrying out the function application,
using let bindings to express the explicit sequencing.
First $f$ is evaluated, followed by $g$, then $a$, before the final application occurs.

The distinction between the strategies has implications for performance.
Passing thunks explicitly to functions in CBN may needlessly duplicate code execution
if those arguments are used multiple times.
Conversely, evaluating arguments before passing them to functions in CBV
may needlessly execute code if those arguments are not used at all.
Choosing one evaluation strategy over another is choosing between these tradeoffs,
and CBPV provides a uniform interface for manipulating code once the decision is made
without introducing behaviour that violates the expected strategy.

To seriously consider CBPV as an IR, we need to think about what happens \emph{after} compiling to CBPV.
At a high level, a compiler IR may undergo some optimization passes,
then compiled to lower-level IRs.
In this paper, we look at passes that affect control flow,
and in particular at commuting conversions.

\subsection{Commuting conversions}

Commuting conversions are syntactic transformations
pushing evaluation contexts into tail positions of eliminators
that preserve evaluation order.
An important benefit of performing commuting conversions is that it exposes inlining opportunities ---
that is, subexpressions that can be reduced to simplify code.
For instance, we can commute a let-bound conditional to reveal direct bindings of returned values.
(We use syntactic sugar for boolean conditionals implemented as sums.)
%
\usetagform{brack}
\begin{align}
  \nonumber
  & [[let b ← (if v then return false else return w) in m]] \\
  \label{eq:let-if}
  \Longrightarrow \ & [[if v then (let b ← return false in m) else (let b ← return w in m)]]
\end{align}
\usetagform{default}

Now we may choose to inline $[[false]]$ inside of $[[m]]$ in place of $[[b]]$
and simplify the then branch.
We may also choose \emph{not} to inline $[[w]]$ in the else branch
if it happens that $[[w]]$ is a particularly large value we don't want to duplicate.
The inlining optimization wouldn't have been possible without the commutation,
and is a well-known technique \citep{join};
this particular commutation is part of \emph{case-floating}.

Following \citet{join}, we extract out from our syntax the evaluation contexts,
which are elimination forms with holes in place of computation scrutinees,
and the tail contexts, which are elimination forms with holes in place of all other computations.
These yield a total of eight commuting conversions,
where four evaluation contexts commute with two tail contexts.
They have straightforward generalizations to more constructs:
for instance, if we were to add value pairs, which are eliminated by pattern matching,
we would add another tail context, producing four more commuting conversions.

\begin{figure}[h]
  \begin{align}
    \tag{evaluation contexts}   [[E]] &\dblcolon= [[let x ← □ in m]] \mid [[□ v]] \mid [[fst □]] \mid [[snd □]] \\
    \tag{tail contexts}         [[L]] &\dblcolon= [[let x ← n in □]] \mid [[case v of inl x ⇒ □ ; inr y ⇒ □]] \\
    \tag{commuting conversions} & [[ E[ L[m] ] ]] \Longrightarrow [[ L[ E[m] ] ]]
  \end{align}
  \caption{Evaluation and tail contexts; commuting conversions}
  \Description[]{}
  \label{fig:commute}
\end{figure}

Another benefit of commuting conversions is that it syntactically sequentializes code,
in the sense that code becomes less nested and more resembles assembly code.
On the right-hand side of \cref{eq:let-if},
we can imagine further compiling the term
into a branching instruction to two code blocks,
each of which assign some value to a register before continuing with the code in $[[m]]$.
This sequentialization is the result of pushing computations into tail positions
in the commuting conversions.

The sequentializing nature of commuting conversions resembles
the A-normalization compiler pass into A-normal form (ANF)
because commuting conversions are in fact the A-reductions that characterize ANF \citep{ANF}.
For the lambda calculus, A-normalization does two things:
it binds intermediate computations,
and it sequentializes computations.
It does the former by including evaluation contexts with holes
in the argument position of function applications
and in the scrutinee position of case expressions.
CBPV doesn't require such contexts since the translation into CBPV
has already either thunked (for CBN) or bound (for CBV) intermediate computations.
All that's left to do is to sequentialize them via commuting conversions.%
\footnote{This is similar to first compiling to monadic normal form (MNF),
which only binds intermediate computations,
then performing sequentialization afterwards \citep{ANF-dead}.}
%
\begin{figure}[h]
  \begin{align}
    [[n]] &\dblcolon= [[v!]] \mid [[λx. m]] \mid [[n v]] \mid [[return v]] \mid [[fst n]] \mid [[snd n]] \tag{computations} \\
    [[m]] &\dblcolon= n \mid [[let x ← n in m]] \mid [[case v of inl x ⇒ m1 ; inr y ⇒ m2]] \tag{configurations}
  \end{align}
  \caption{Commuting conversion normal form of computations and configurations}
  \Description[]{}
  \label{fig:ccnf}
\end{figure}

Rather than performing commuting conversions one by one,
we present a one-pass transformation into a normal form
with respect to all commuting conversions (CCNF) in \cref{sec:ccnf} % TODO: Is this really the best name?
% \footnote{Compiling with Continuations, Continued also calls them cc-normal form.}
that resembles the usual one-pass transformation into ANF \citep{ANF}.
To borrow terminology from ANF,
we call $[[E]]$s that are filled with terms free of $[[L]]$s \emph{computations},
while we call filled $[[L]]$s \emph{configurations},
using $[[n]]$ for such computations only and $[[m]]$ for configurations.
\Cref{fig:ccnf} explicitly lists the syntax obtained from these fillings;
values remain the same, where thunks may contain configurations.

Because the commuting conversions only introduce new computations in tail position,
\emph{new} opportunities for inlining only occur in tail position,
so performing those new inlinings will not violate CCNF.
However, subsequent inlinings that involve forcing direct thunks may violate CCNF.
Consider the following sequence of a commutation followed by an inlining that reduces a function.
%
\begin{align}
  &[[(let x ← n1 in (λy'. let y ← y'! in m1)) {let z ← n2 in m2}]] \nonumber \\
  &\Longrightarrow [[let x ← n1 in ((λy'. let y ← y'! in m1) {let z ← n2 in m2})]] \tag{\textit{commute}} \\
  &\Longrightarrow [[let x ← n1 in (let y ← {let z ← n2 in m2}! in m1)]] \tag{\textit{inline}}
\end{align}

The resulting term is in CCNF, but if we force the thunk,
we end up with a nested let expression, which is not in CCNF.
Therefore, renormalization is required after cascading inlinings that involve forcing thunks.

\subsection{Join points}

There is a glaring code optimization issue with the commuting conversion in \cref{eq:let-if},
and in general with commuting let-bound case expressions:
the let body $[[m]]$ gets duplicated across the branches of the case expression.
%
\usetagform{brack}
\begin{align}
  \nonumber
  &[[let x ← (case v of inl y1 ⇒ m1 ; inr y2 ⇒ m2) in m]] \\
  \label{eq:duped}
  \Longrightarrow \ &[[case v of inl y1 ⇒ (let x ← m1 in m) ; inr y2 ⇒ (let x ← m1 in m)]]
\end{align}
\usetagform{default}

If the size of $[[m]]$ is very large, this can cause code bloat,
especially if the branches contain further case expressions.
The usual solution for the lambda calculus with let expressions
is to let-bind a closure containing $[[m]]$ to be called at the end of the branch,
also known as a \emph{join point}.
Similarly, in CBPV, we can bind a thunked function to be forced and applied.
%
\usetagform{brack}
\begin{align}
  & \nonumber {\color{gray} [[let x ← (case v of inl y1 ⇒ m1 ; inr y2 ⇒ m2) in m]]} \\
  \label{eq:let-join} \Longrightarrow \ &[[let z ← return {λx. m} in _]] \\
  & \nonumber \phantom{\kw{let} \gap}
  [[case v of inl y1 ⇒ (let x ← m1 in z! x) ; inr y2 ⇒ (let x ← m2 in z! x)]]
\end{align}
\usetagform{default}

To the next compiler passes that see this code,
the thunk is a value that may capture variables and escape its scope,
so somewhere along the pipeline, the thunk will be converted into a closure and lifted out,
and $[[z! x]]$ will correspond to a function call.
However, we know from the commuting conversion that the thunk will never escape its scope,
since it's never passed to a function or stored in another thunk;
all that we do to it is force it and apply the function within.
Its purpose is only to join up branches of a computation,
and should be compiled to a local code block that is jumped to.
%
\begin{figure}[h]
  \begin{align}
    m &\dblcolon= \dots \mid [[join j x = m in m]] \mid [[jump j v]]
    \tag{join points, jumps}
  \end{align}
  \caption{Extended configurations}
  \Description[]{}
  \label{fig:join}
\end{figure}

Inspired by \citet{join},
who tackle the same issue with commuting conversion in System F with case expressions,
we too add explicit join point and jump constructs to CBPV in \cref{fig:join}.
They are accompanied by typing rules that restrict where join points may be used,
which we cover in \cref{sec:cbpv}.
In contrast to \opcit,
our jumps are configurations and may only appear in tail position,
as commuting conversion normalization discards contexts around jumps.
All in all, these restrictions ensure that join points are only ever used by local tail jumps.

Coming back to the commuting conversion of \cref{eq:let-join},
we use join points in place of the bound thunk,
jumping to them after binding the branches.
%
\usetagform{brack}
\begin{align}
  & \nonumber {\color{gray} [[let x ← (case v of inl y1 ⇒ m1 ; inr y2 ⇒ m2) in m]]} \\
  \Longrightarrow \ &[[join j x = m in _]] \\
  & \nonumber \phantom{\kw{join} \gap}
  [[case v of inl y1 ⇒ (let x ← m1 in jump j x) ; inr y2 ⇒ (let x ← m2 in jump j x)]]
\end{align}
\usetagform{default}

Unfortunately, even with join points,
the case-in-case optimizations highlighted in \opcit are not performed,
as the translation to CBPV has already obscured these opportunities.
Case-in-case optimizations deal with case expressions in lambda calculus
whose scrutinees are themselves case expressions,
where a commuting conversion followed by an inlining simplifies away an inner case analysis.
Consider the following example in lambda calculus with conditionals,
where the left-hand side would be optimized with join points to the right-hand side.
%
\usetagform{brack}
\begin{align}
  &[[:concrete: if (if v then false else w) then m_1 else m_2]] \nonumber \\
  \Longrightarrow \ &[[join j = m2 in (if v then jump j else (if w then m1 else jump j))]]
\end{align}
\usetagform{default}

If we translate the left-hand side for both CBN and CBV to CBPV,
the inner conditional is bound in an intermediate let expression,
making its case-in-case nature difficult to identify.%
\footnote{\Citet{ANF-dead} also highlights this difficulty in the context of MNF.}
Further sequentialization (and inlining) with join points prevents duplication of $[[m1]]$ and $[[m2]]$,
but contains an extra conditional inside the join point,
which is unnecessary in the branch where $[[v]]$ is $[[true]]$
and we know the next computation to execute must be $[[m2]]$.
%
\begin{align}
  & {\color{gray} [[:concrete: if (if v then false else w) then m_1 else m_2]]} \nonumber \\
  \Longrightarrow \ & [[let x ← (if v then return false else return w) in (if x then m1 else m2)]] \tag{\textit{CBPV}} \\
  \Longrightarrow \ & [[join j x = (if x then m1 else m2) in _]] \tag{\textit{commute, inline}} \\
  & \phantom{\kw{join} \gap} [[(if v then jump j false else jump j w)]] \nonumber
\end{align}

Even so, join points still successfully solve the problem of preventing
duplication of $[[m1]]$ and $[[m2]]$ without creating a new thunk.

\subsection{Technical contributions}

The technical contributions of this paper are as follows:

\begin{itemize}
  \item We identify a subset of CBPV that is normal with respect to
    all commuting conversions that syntactically sequentialize computations.
    To this subset, we add join point and jump constructs,
    and design a type system enforcing non-escaping join points and tail-only jumps.
    $\hookrightarrow$~\cref{sec:cbpv}
  \item We define a single-pass transformation from CBPV into our extended normal form,
    using joins and jumps to avoid duplicating computations.
    This transformation preserves well-typedness,
    proven straightforwardly by induction.
    $\hookrightarrow$~\cref{sec:ccnf}
  \item We show that the transformation preserves evaluation behaviour:
    a closed term of value type with no thunks
    and its transformation must evaluate to the same terminal value.
    This is proven via a logical equivalence on terms,
    and requires showing that commuting conversions are logically equivalent.
    $\hookrightarrow$~\cref{sec:proof}
\end{itemize}

\section{CBPV with Join Points} \label{sec:cbpv}

While \cref{sec:overview} presents the source (plain CBPV) and target (CCNF CBPV with join points)
languages with distinct syntactic forms,
in our mechanization (and thus our technical presentation here),
we use a single unified syntax and treat CC-normalization as a source-to-source translation,
showing \aposteriori in \cref{sec:ccnf} that the output of the translation satisfies \cref{fig:ccnf,fig:join}.
In \cref{sec:proof}, we reason about equivalence between a CBPV term and its translation,
which requires both sides of the equivalence to belong to the same sytnactic category
for the equivalence to have the properties of an equivalence relation.

\begin{figure}[h]
  \begin{align}
    [[A]] &\dblcolon= [[⊤]] \mid [[A + A]] \mid [[U B]] \tag{value types} \\
    [[B]] &\dblcolon= [[A → B]] \mid [[B & B]] \mid [[F A]] \tag{computation types} \\
    [[v]], [[w]] &\dblcolon= [[x]] \mid [[()]] \mid [[inl v]] \mid [[inr v]] \mid [[{m}]] \tag{values} \\
    [[m]], [[n]] &\dblcolon= [[v!]] \mid [[λx. m]] \mid [[n v]] \mid [[⟨m, m⟩]] \mid [[fst n]] \mid [[snd n]] \mid [[return v]] \tag{computations} \\
    &\mid [[let x ← n in m]] \mid [[case v of inl x ⇒ m ; inr y ⇒ m]] \nonumber \\
    &\mid [[join j x = m in m]] \mid [[jump j v]] \nonumber
  \end{align}
  \caption{Syntax of value, computations, and their types}
  \Description[]{}
  \label{fig:syntax-full}
\end{figure}

\Cref{fig:syntax-full} lists the full syntax of values and computations,
along with value types and computation types.
For clarity, although the mechanization uses de Bruijn indexing and simultaneous substitutions,
we present the syntax here in nominal form,
with \fbox{$[[v{x ↦ w}]]$}, \fbox{$[[m{x ↦ w}]]$} denoting single (capture-avoiding) substitution
of $[[x]]$ for $[[w]]$ in $[[v]]$ and $[[m]]$, respectively.

\begin{figure}[h]
  \begin{align}
    [[G]] &\dblcolon= [[•]] \mid [[G, x : A]] & [[s]], [[r]] &\dblcolon= [[•]] \mid [[s, x ↦ v]] \tag{value contexts; substitution maps} \\
    [[D]] &\dblcolon= [[•]] \mid [[D, j : A ↗ B]] & [[p]], [[q]] &\dblcolon= [[•]] \mid [[p, j x = m]] \tag{join contexts; join stacks}
  \end{align}
  \caption{Contexts, substitution maps, and join stacks}
  \Description[]{}
  \label{fig:ctxt-subst}
\end{figure}

\subsection{Evaluation semantics}

\subsection{Typing rules}

\section{Commuting Conversion Normalization} \label{sec:ccnf}

\begin{figure}[h]
  \begin{align*}
    [[K]] &\dblcolon= [[□]] \mid [[let x ← □ in m]] \mid [[ K[□ v] ]] \mid [[ K[fst □] ]] \mid [[ K[snd □] ]]
  \end{align*}
  \caption{Commuting conversion normalization continuation}
  \Description[]{}
  \label{fig:kont}
\end{figure}

\section{Commuting Conversion Normalization with Join Points Preserves Evaluation} \label{sec:proof}

Normalizing by all commuting conversions shouldn't affect the meaning of a program.
Formally, if a closed computation evaluates to a returned value,
then its translation runs to the same value.
Because we don't evaluate inside of thunks,
we consider only computations that return \emph{ground} values,
which are those of type $[[T]] \dblcolon= [[⊤]] \mid [[T + T]]$.
Otherwise, a thunk and its CCNF are values that don't evaluate any further,
but aren't necessarily syntactially equal.

\begin{theorem} \label{thm:ground-run}
  Given $[[m]]$ such that $[[• | • ⊢ m : F T]]$,
  if $[[m ⇓ return v]]$, then $[[⟦ m ⟧ ⇓ return v]]$.
\end{theorem}

We prove this property as a corollary of a logical equivalence between a computation and its translation.
This machinery is required because the simpler method using a simulation argument,
such as the following statement, unfortunately doesn't work.

\begin{fail}[Simulation]
  If $[[m ⇝ n]]$ then $[[⟦m⟧ ⇝* ⟦n⟧]]$.
\end{fail}
\begin{proof}[Counterexample]
  Suppose we have CCNF computations $[[n1]]$, $[[n2]]$ and configuration $[[n]]$.
  Consider the term $[[let x ← {let y ← n1 in n2}! in m]]$,
  which reduces to $[[let x ← (let y ← n1 in n2) in m]]$.
  The left-hand side translates to itself, since it's already in CCNF,
  while the right-hand side translates to $[[let y ← n1 in let x ← n2 in m]]$.
  Therefore, we need to show that the right-hand side reduces to its own translation,
  but there is no such reduction sequence since there is no reduction step that commutes let bindings.
\end{proof}

However, if we know that $[[n1]]$ in the counterexample reduces to some $[[return v]]$,
then we can deduce that the right side and its translation must reduce to
$[[let x ← n2{y ↦ v} in m]]$, which gives us an equivalence between the original term and its translation.
If our counterexample is well typed,
then the logical equivalence gives us precisely the required information that
subterms must reduce to canonical terms such as returned values.

The proof of \cref{thm:ground-run} proceeds by:
\begin{enumerate}
  \item \label{item:logrel} Defining a standard logical equivalence over CBPV types;
  \item \label{item:semeq} Closing over term and join point contexts with a semantic equivalence;
  \item \label{item:fundamental} Proving the fundamental theorem this equivalence,
    namely that well-typed terms are semantically equivalent to themselves;
  \item \label{item:cc} Showing that well-typed commuting conversions are in the semantic equivalence;
  \item \label{item:kont} Defining semantic equivalence of continuations
    and proving its fundamental theorem;
  \item \label{item:plug} Showing that plugging continuations respects semantic equivalence; and finally,
  \item \label{item:end} Proving that given a well-typed computation and a well-typed continuation,
    plugging the continuation with the computation
    is equivalent to translating the computation using the continuation.
\end{enumerate}
The theorem then holds by instantiating with the empty continuation.
\Cref{sec:proof:logrel} covers \crefrange{item:logrel}{item:fundamental},
\cref{sec:proof:cc} covers \cref{item:cc},
\cref{sec:proof:plug} covers \crefrange{item:kont}{item:plug},
and \cref{sec:proof:end} covers \cref{item:end}.

\subsection{Logical equivalence and the fundamental theorem} \label{sec:proof:logrel}

\subsection{Semantic equivalence of commuting conversions} \label{sec:proof:cc}

\subsection{Semantic equivalence of plugging continuations} \label{sec:proof:plug}

\subsection{Commuting conversion normalization is semantically equivalent to plugging} \label{sec:proof:end}

\section{Related Work} \label{sec:related}

\Citet{CBPV-Coq} mechanize in Rocq a vast amount of metatheory for call-by-push-value,
including normalization and observational equivalence.
Our work builds on their design of logical equivalence between terms.
They show a number of commuting conversions as semantic equivalences,
namely commuting let-bound let expressions (Lemma 8.4, Equation 5)
and function application of let expressions (Lemma 8.6, Equation 1),
but are not comprehensive in listing all possible commuting conversions systematically
as we have done in (\TODO: eventually).

Our join points and their typing judgements with a separate join point typing context
are inspired by \citet{join}, who add join points and jumps to System F,
and implement their system in the Haskell compiler GHC.
We simplify the addition by only allowing jumps in tail position,
while they permit jumps in evaluation contexts.
They perform their optimizations equation by equation,
which yields intermediate steps that require this permissiveness,
such as the following.
%
\begin{align*}
  [[(join j x = m in jump j v) w]] &\Longrightarrow [[join j x = m w in (jump j v) w]] \\
                                   &\Longrightarrow [[join j x = m w in jump j v]]
\end{align*}
Because we present instead a single-pass algorithm,
we never produce such intermediate steps that need to be typeable,
so we are able to eliminate them from our syntax outright.
Doing so also simplifies typing:
their jumps need to be typeable with arbitrary types,
which they implement using type polymorphism,
while our jumps are always in tail position
and fixed to the return type of the join point they jump to.

Instead of specialized join and jump constructs,
\citet{wowo} use control operators that bind second-class continuations,
along with a type system that distinguishes between first- and second-class values.
Their system allows for both direct style and continuation-passing style optimizations;
this is not directly applicable to CBPV, which is already in direct style
by virtue of binding intermediate computations.
However, the corresponding dual of CBPV is stack-passing style \citep{compiling},
and it would be interesting to see whether a similar technique could be applied using second-class stacks.

\TODO: compare to \url{https://github.com/zydeco-lang/zydeco}

\section{Conclusion and Future Work} \label{sec:future}

\begin{itemize}
  \item \TODO: compiling to assembly \citep{CBPV-STAL} with and without CC-normalization optimizations
  \item \TODO: measuring stack size using cost model \citep{cost}
  \item \TODO: actual effects
  \item \TODO: real-life evaluation (for PLDI?)
\end{itemize}

\iffalse
\begin{align}
  [[let x ← (let y ← m1 in m2) in m3]]
  & \Longrightarrow [[let y ← m1 in let x ← m2 in m3]]
  \tag{let--let} \\
  [[(let x ← n in m) v]]
  & \Longrightarrow [[let x ← n in m v]]
  \tag{let--app} \\
  [[fst (let x ← n in m)]]
  & \Longrightarrow [[let x ← n in fst m]]
  \tag{let--fst} \\
  [[snd (let x ← n in m)]]
  & \Longrightarrow [[let x ← n in snd m]]
  \tag{let--snd} \\
  [[let x ← (case v of inl y ⇒ m1 ; inr z ⇒ m2) in m]]
  & \Longrightarrow \nonumber \\
  \mathclap{[[case v of inl y ⇒ let x ← m1 in m ; inr z ⇒ let x ← m2 in m]]}
  \tag{case--let} \\
  [[(case v of inl x ⇒ m1 ; inr y ⇒ m2) w]]
  & \Longrightarrow \nonumber \\
  \mathclap{[[case v of inl x ⇒ m1 w ; inr y ⇒ (# m2 w #)]]}
  \tag{case--app} \\
  [[fst (case v of inl x ⇒ m1 ; inr y ⇒ m2)]]
  & \Longrightarrow \nonumber \\
  \mathclap{[[case v of inl x ⇒ fst m1 ; inr y ⇒ fst m2]]}
  \tag{case--fst} \\
  [[snd (case v of inl x ⇒ m1 ; inr y ⇒ m2)]]
  & \Longrightarrow \nonumber \\
  \mathclap{[[case v of inl x ⇒ snd m1 ; inr y ⇒ snd m2]]}
  \tag{case--snd}
\end{align}
\fi

\bibliography{main}

\end{document}

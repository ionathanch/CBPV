\newif\ifarxiv
\arxivfalse

% acmart now uses unicode-math, and mathtools needs to be loaded before it
\RequirePackage{mathtools}
\documentclass[acmsmall,screen,review,anonymous,nonacm]{acmart}
\settopmatter{printacmref=false, printccs=false, printfolios=true}

\ifarxiv
\pdfoutput=1
\nolinenumbers
\usepackage[T1]{fontenc}
\usepackage[scale=0.92]{inconsolata}
\else
\usepackage{fontspec}
\setmonofont[Scale=MatchLowercase]{inconsolata}
\fi

\usepackage[supertabular]{ottalt}
\let\newlist\relax
\let\renewlist\relax
\usepackage{thmtools} % fix cref lemma/corollary names; load BEFORE cleveref
\usepackage[capitalize,nameinlink,noabbrev]{cleveref}
\usepackage{enumitem,booktabs,subcaption,xspace,doi}
\usepackage{mathpartir,stmaryrd,colonequals}
\usepackage[bottom,flushmargin,multiple,para]{footmisc} % para spacing is weird and ugly

\newcommand{\repo}{https://github.com/ionathanch/CBPV/tree/join}
\newcommand{\lang}{CBPV$_{\mathrm{j}}$\@\xspace}
\newcommand{\titlebreak}{\texorpdfstring{\\}{}}
\newcommand{\TODO}{\textcolor{red}{\textbf{\textsf{TODO}}}\@\xspace}
\newcommand{\ie}{\textit{i.e.}\@\xspace}
\newcommand{\eg}{\textit{e.g.}\@\xspace}
\newcommand{\etal}{\textit{et al.}\@\xspace}
\newcommand{\opcit}{\textit{op. cit.}\@\xspace}
\newcommand{\vs}{\textit{vs.}\@\xspace}
\newcommand{\ala}{\textit{\`a la}\@\xspace}
\newcommand{\apriori}{\textit{a priori}\@\xspace}
\newcommand{\aposteriori}{\textit{a posteriori}\@\xspace}
\newcommand{\fstar}{F$^\star$\@\xspace}
\newcommand{\welltyped}{well-\hspace{0pt}typed\@\xspace}
\newcommand{\wellfounded}{well-\hspace{0pt}founded\@\xspace}
\newcommand{\wellfoundedness}{well-\hspace{0pt}foundedness\@\xspace}
\newcommand{\wellformedness}{well-\hspace{0pt}formedness\@\xspace}
\newcommand{\welldefinedness}{well-\hspace{0pt}definedness\@\xspace}
\newcommand{\crude}{crude-\hspace{0pt}but-\hspace{0pt}effective\@\xspace}
\newcommand{\Acom}{???\@\xspace}
\newcommand{\highlight}[1]{\colorbox{pink}{#1}}

\newcommand{\thmref}[2]{%
  $\langle$\textnormal{\texttt{\href{\repo/tree/main/src/#1}{#1}:#2}}$\rangle$%
}

\newtheorem{fail}{Falsehood}[section]

\setlength{\fboxsep}{1.5pt}
\setlength{\abovecaptionskip}{0.25\baselineskip}
\setlength{\floatsep}{\baselineskip}
\setlength{\textfloatsep}{\baselineskip}
\setlength{\intextsep}{0.25\baselineskip}
\setlength{\jot}{0\baselineskip}

\setlist[itemize]{leftmargin=2\parindent}
\setlist[enumerate]{topsep=0pt}
\setlist[enumerate,1]{label=\arabic*.,ref=\arabic*}

\mathtoolsset{showmanualtags}
\newtagform{brack}{[}{]}
\urlstyle{tt}

\crefformat{enumi}{#2step~#1#3}
\Crefformat{enumi}{#2step~#1#3}
\crefrangeformat{enumi}{step~#3#1#4 to #5#2#6}
\Crefrangeformat{enumi}{step~#3#1#4 to #5#2#6}
\crefformat{equation}{#2Equation~#1#3}
\Crefformat{equation}{#2Equation~#1#3}
\crefmultiformat{equation}{Equations~#2#1#3}{ and #2#1#3}{, #2#1#3}{ and #2#1#3}
\Crefmultiformat{equation}{Equations~#2#1#3}{ and #2#1#3}{, #2#1#3}{ and #2#1#3}

\citestyle{acmauthoryear}
\bibliographystyle{ACM-Reference-Format}

\title[]{Commuting Conversions and Join Points \titlebreak for Call-By-Push-Value}

\author{Jonathan Chan}
\orcid{0000-0003-0830-3180}
\affiliation{University of Pennsylvania}
\email{jcxz@seas.upenn.edu}

\author{Madi Gudin}
\affiliation{Amherst College}
\email{mgudin27@amherst.edu}

\author{Annabel Levy}
\affiliation{University of Maryland, Baltimore County}
\email{alevy2@umbc.edu}

\author{Stephanie Weirich}
\orcid{0000-0002-6756-9168}
\affiliation{University of Pennsylvania}
\email{sweirich@seas.upenn.edu}

\acmJournal{PACMPL}
\acmConference[short]{name}{date}{venue}
\acmVolume{}
\acmNumber{}
\acmArticle{}
\acmYear{}
\acmMonth{}
\acmISBN{}
\acmDOI{}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011008.10011009.10011012</concept_id>
       <concept_desc>Software and its engineering~Functional languages</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10011007.10011006.10011008.10011024.10011027</concept_id>
       <concept_desc>Software and its engineering~Control structures</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Software and its engineering~Functional languages}
\ccsdesc[500]{Software and its engineering~Control structures}

\inputott{rules}

\begin{document}

\setlength{\abovedisplayskip}{0.25\baselineskip}
\setlength{\belowdisplayskip}{0.25\baselineskip}

\begin{abstract}

\end{abstract}

\maketitle

\section{Introduction}

\TODO: introduction

Call-by-push-value (CBPV) \citep{CBPV} is a language with a strong distinction
between \emph{values} and \emph{computations} in its syntax and types.
The benefit of the distinction is that its constructs precisely describe
when computations are executed and when they are suspended as values.
% isolating purity, advantages other than just CBN + CBV
With these constructs, CBPV subsumes both call-by-name (CBN) and call-by-value (CBV)
evaluation strategies of the lambda calculus,
in the sense that it represents lambda calculus terms with intended CBN or CBV semantics
as terms with different evaluation behaviour.

By separating out values from computations,
CBPV is a suitable setting for reasoning about program equivalences where evaluation order matters.
\Citet{GTT}, for instance, use $\eta$-equivalences that only hold in CBN or in CBV languages
to reason about casts in gradual typing,
and do so in a CBPV setting since it subsumes those equivalences.
In turn, $\eta$-equivalences justify compiler optimizations
such as dead branch elimination and constant subexpression elimination/constant folding,
and \citet{equational} use CBPV to verify these kinds of optimizations.

Furthermore, the syntactically explicit evaluation order of CBPV makes it low-level enough
to be suitable as a bona fide compiler intermediate representation (IR),
and not merely for modelling compiler optimizations.
In particular, \citet{CFG} show an equivalence between CBPV
and control flow graph representations (CFG) in single static assignment (SSA) form,
and thus the viability of CBPV as an IR for imperative languages in lieu of SSA-form CFGs.
In the functional setting, the clear boundary between computations and values
mediated by thunked computations as values
makes it clear where closures ought to be created during compilation,
so \citet{closures} augment CBPV with explicit closures
and reason about optimizations related to closure conversion.

\TODO: Sentence to connect above references to why we care about looking at more CBPV compiler transformations.
In this paper, we focus on a set of \emph{commuting conversion} compiler transformations,
which are source-to-source transformations in CBPV
that rearrange the syntax of programs without affecting evaluation order.
Such transformations are often coupled with others that bind intermediate computations,
such as the A-reductions that characterize A-normal form (ANF) \citep{ANF}.
However, CBPV already binds intermediate computations,
so we can isolate just the commuting conversions.
The benefit of commuting conversions is twofold:
as a compiler optimization, they expose opportunities for program inlining,
and as a compiler pass, they unnest computations to more resemble the sequential nature of low-level code.

An issue with na\"ively applying commuting conversions,
in particular in the presence of branching computations,
is that code may be duplicated;
applying many commuting conversions can then cause exponential code bloat.
To resolve this issue, we adapt the join point constructs by \citep{join}
from the lambda calculus to CBPV and adapt the type system to accommodate them.
Introducing specialized constructs rather than using existing ones
allows us to restrict their usage to only where they are needed,
which can lead to more efficient compilation later on.

This work presents a normal form for CBPV with respect to commuting conversions,
and a single-pass transformation into this normal form using join points.
We prove that this transformation is type-preserving and evaluation-preserving:
it does not change the meaning of CBPV programs.
These proofs are mechanized in Lean 4 \citep{lean}, and the proof development is provided in the supplementary materials.
In the next section, we give an overview of the language, the transformation, and its motivation,
leading to the following contributions.

\begin{itemize}
  \item We identify a subset of CBPV that is normal with respect to
    all commuting conversions that syntactically unnest computations.
    To this subset, we add join point and jump constructs,
    and design a type system enforcing non-escaping join points and tail-only jumps.
    $\hookrightarrow$~\cref{sec:cbpv}
  \item We define a single-pass transformation from CBPV into our extended normal form,
    using joins and jumps to avoid duplicating computations.
    This transformation preserves well-typedness,
    proven straightforwardly by induction.
    $\hookrightarrow$~\cref{sec:ccnf}
  \item We show that the transformation preserves evaluation behaviour:
    a closed term of value type with no thunks
    and its transformation must evaluate to the same terminal value.
    This is proven via a logical equivalence on terms,
    and requires showing that commuting conversions are logically equivalent.
    $\hookrightarrow$~\cref{sec:proof}
\end{itemize}
%
We discuss related work in \cref{sec:related},
and conclude with future work in \cref{sec:future}.

\section{Overview} \label{sec:overview}

The core ideas of this paper begin with the thesis that CBPV \citep{CBPV}
is suitable as a compiler IR because it represents control flow explicitly.
In particular, it subsumes both CBN and CBV semantics of the lambda calculus:
compiling a lambda calculus term with the CBN or CBV compilation strategy yields different CBPV terms
whose execution mirrors that of the original evaluation strategy.

\begin{figure}[h]
  \begin{align}
    [[v]], [[w]] &\dblcolon= [[x]] \mid [[()]] \mid [[inl v]] \mid [[inr v]] \mid [[{m}]] \tag{values} \\
    [[m]], [[n]] &\dblcolon= [[v!]] \mid [[λx. m]] \mid [[n v]] \mid [[⟨m, m⟩]] \mid [[fst n]] \mid [[snd n]] \mid [[return v]] \tag{computations} \\
    &\mid [[let x ← n in m]] \mid [[case v of inl x ⇒ m ; inr y ⇒ m]] \nonumber
  \end{align}
  \caption{Syntax of call-by-push-value values and computations}
  \Description[]{}
  \label{fig:syntax}
\end{figure}

CBPV syntactically distinguishes values and computations, listed in \cref{fig:syntax},
using explicit thunks to turn suspended computations into values
and explicit returns to embed values into computations,
which the CBN and CBV translations use in different ways to enforce when computation occurs.
Alongside the thunk and return constructs,
we include the unit value, value sums, functions, and computation pairs.%
\footnote{For simplicity, we omit value pairs, which are present in Levy's original CBPV;
they are interesting in our setting because the eliminator is pattern matching,
but we already have pattern matching on value sums to consider.}

As an example of the way CBN and CBV translations differ,
consider the lambda calculus term $[[(λx. x) ((λy. y) z)]]$,
which translates to the following two CBPV terms.
%
\begin{align*}
  \textbf{CBN} &\quad [[:concrete: (λx. x!) {(λy. y!) {z!}}]] \\
  \textbf{CBV} &\quad [[:concrete: let f ← return {λx. return x} in let a ← (let g ← return {λy. return y} in g! z) in f! a]]
\end{align*}

While both terms evaluate to the same final value $[[z]]$,
their evaluation sequences are different.
In the CBN translation, function arguments are thunked and passed wholesale,
then forced as they are needed.
In the CBV translation, the function and the argument are evaluated in order
before carrying out the function application,
using let bindings to express the explicit sequencing.
First $f$ is evaluated, followed by $g$, then $a$, before the final application occurs.

% The distinction between the strategies has implications for performance.
% Passing thunks explicitly to functions in CBN may needlessly duplicate code execution
% if those arguments are used multiple times.
% Conversely, evaluating arguments before passing them to functions in CBV
% may needlessly execute code if those arguments are not used at all.
% Choosing one evaluation strategy over another is choosing between these tradeoffs,
% and CBPV provides a uniform interface for manipulating code once the decision is made
% without introducing behaviour that violates the expected strategy.

To seriously consider CBPV as an IR, we need to think about what happens \emph{after} compiling to CBPV.
At a high level, a compiler IR may undergo some optimization passes,
then be compiled to lower-level IRs.
In this paper, we look at passes that affect control flow,
and in particular at commuting conversions.

\subsection{Commuting conversions}

Commuting conversions are syntactic transformations
pushing evaluation contexts into tail positions of eliminators
that preserve evaluation order.
An important benefit of performing commuting conversions is that it exposes inlining opportunities ---
that is, subexpressions that can be reduced to simplify code.
For instance, we can commute a let-bound conditional to reveal direct bindings of returned values.
(We use syntactic sugar for boolean conditionals implemented as sums.)
%
\usetagform{brack}
\begin{align}
  \nonumber
  & [[let b ← (if v then return false else return w) in m]] \\
  \label{eq:let-if}
  \Longrightarrow \ & [[if v then (let b ← return false in m) else (let b ← return w in m)]]
\end{align}
\usetagform{default}

Now we may choose to inline $[[false]]$ inside of $[[m]]$ in place of $[[b]]$
and simplify the then branch.
We may also choose \emph{not} to inline $[[w]]$ in the else branch
if it happens that $[[w]]$ is a particularly large value we don't want to duplicate.
The inlining optimization wouldn't have been possible without the commutation,
and is a well-known technique \citep{join};
this particular commutation is part of \emph{case-floating}.

Following \citet{join}, we define commuting conversions as all transformations
that push elimination forms inside of blocking computations.
\Cref{eq:let-if} pushes let bindings into conditionals;
as shown below, we can also push let bindings into other let bindings,
as well as function applications into let bindings and conditionals,
to expose those inlining opportunities.
%
\usetagform{brack}
\begin{align}
  \label{eq:let-let}
  [[let x ← (let y ← n in return v) in m]] &\Longrightarrow [[let y ← n in (let x ← return v in m)]] \\
  \label{eq:app-let}
  [[(let x ← n in λy. m) v]] &\Longrightarrow [[let x ← n in (λy. m) v]] \\
  \label{eq:app-if}
  [[(if w then (λx. m1) else (λx. m2)) v]] &\Longrightarrow [[if w then (λx. m1) v else (λx. m2) v]]
\end{align}
\usetagform{default}

Another benefit of commuting conversions is that it unnests expressions,
bringing it closer to lower-level code.
Unnested expressions are easier to compile because control flow follows the shape of the syntax.
In \cref{eq:let-let}, finding the next computation to execute on the left-hand side requires
traversing into the $[[x]]$ binding, then evaluating the $[[y]]$ binding,
and finally popping back out to evaluate the body $[[m]]$.
In contrast, the right-hand side makes it explicit that we first evaluate the $[[y]]$ binding,
then the $[[x]]$ binding, then the body.

This sequential nature of commuting conversions resembles
the A-normalization compiler pass into A-normal form (ANF)
because commuting conversions are part of the A-reductions that characterize ANF \citep{ANF}.
For the lambda calculus, A-normalization makes control flow syntactically explicit by doing two things:
it binds intermediate computations, and it sequentializes computations.
Translations from the lambda calculus to CBPV already bind intermediate computations,
so all that's left to do is to sequentialize them via commuting conversions.%
\footnote{This is similar to first compiling to monadic normal form (MNF),
which only binds intermediate computations,
then performing sequentialization afterwards \citep{ANF-dead}.
\TODO: Move this to related work.}
%
\begin{figure}[h]
  \begin{align}
    [[n]] &\dblcolon= [[v!]] \mid [[λx. m]] \mid [[n v]] \mid [[return v]] \mid [[⟨m, m⟩]] \mid [[fst n]] \mid [[snd n]] \tag{\Acom} \\
    [[m]] &\dblcolon= n \mid [[let x ← n in m]] \mid [[case v of inl x ⇒ m1 ; inr y ⇒ m2]] \tag{computations}
  \end{align}
  \caption{Commuting conversion normal form of computations}
  \Description[]{}
  \label{fig:ccnf}
\end{figure}

Rather than performing commuting conversions one by one,
we present a one-pass transformation into a normal form
with respect to all commuting conversions (CCNF) in \cref{sec:ccnf} % TODO: Is this really the best name?
% \footnote{Compiling with Continuations, Continued also calls them cc-normal form.}
that resembles the usual one-pass transformation into ANF \citep{ANF}.
In ANF, terms are divided into values, computations, and configurations,
where computations are a subset of configurations that don't include
let expressions and conditionals (or case expressions generally).
\Cref{fig:ccnf} makes the same distinction for CBPV, where values remain unchanged;
to avoid confusion, we continue calling $[[m]]$s \emph{computations},
while we call $[[n]]$s \emph{\Acom}.

Inlining in $[[n]]$ positions in general doesn't preserve CCNF.
Because commuting conversions push computations only into
the bodies of let expressions and the branches of case expressions,
\emph{new} opportunities for inlining only occur in $[[m]]$ positions,
so performing those new inlinings won't violate CCNF.
However, subsequent inlinings that involve forcing direct thunks may violate CCNF.
Consider the following sequence of a computation followed by an inlining that reduces a function.
%
\begin{align}
  &[[(let x ← n1 in (λy'. let y ← y'! in m1)) {let z ← n2 in m2}]] \nonumber \\
  &\Longrightarrow [[let x ← n1 in ((λy'. let y ← y'! in m1) {let z ← n2 in m2})]] \tag{\textit{commute}} \\
  &\Longrightarrow [[let x ← n1 in (let y ← {let z ← n2 in m2}! in m1)]] \tag{\textit{inline}}
\end{align}

The resulting term is in CCNF, but if we force the thunk,
we end up with a nested let expression, which is not in CCNF.
Therefore, renormalization may be required only after forcing thunks
that appear in $[[n]]$ positions as a result of inlining.

\subsection{Join points}

Generalizing \cref{eq:let-if} to case expressions and arbitrary computations,
the corresponding commuting conversion pushes let bindings into case branches.
%
\usetagform{brack}
\begin{align}
  \nonumber
  &[[let x ← (case v of inl y1 ⇒ n1 ; inr y2 ⇒ n2) in m]] \\
  \label{eq:let-case}
  \Longrightarrow \ &[[case v of inl y1 ⇒ (let x ← n1 in m) ; inr y2 ⇒ (let x ← n1 in m)]]
\end{align}
\usetagform{default}

There is a code optimization issue with \cref{eq:let-if,eq:let-case}:
the let body $[[m]]$ gets duplicated across the branches.
If the size of $[[m]]$ is very large, this can cause code bloat,
especially if the branches contain further case expressions.
The usual solution for the lambda calculus with let expressions
is to let-bind a closure containing $[[m]]$ to be called at the end of the branch,
also known as a \emph{join point}.
Similarly, in CBPV, we can bind a thunked function to be forced and applied.
%
\usetagform{brack}
\begin{align}
  & \nonumber {\color{gray} [[let x ← (case v of inl y1 ⇒ m1 ; inr y2 ⇒ m2) in m]]} \\
  \label{eq:let-join} \Longrightarrow \ &[[let z ← return {λx. m} in _]] \\
  & \nonumber \phantom{\kw{let} \gap}
  [[case v of inl y1 ⇒ (let x ← m1 in z! x) ; inr y2 ⇒ (let x ← m2 in z! x)]]
\end{align}
\usetagform{default}

To the next compiler passes that see this code,
the thunk is a value that may capture variables and escape its scope,
so somewhere along the pipeline, the thunk will be converted into a closure and lifted out,
and $[[z! x]]$ will correspond to a function call.
However, we know from the commuting conversion that the thunk will never escape its scope,
since it's never passed to a function or stored in another thunk;
all that we do to it is force it and apply the function within.
Its purpose is only to join up branches of a computation,
and should be compiled to a local code block that is jumped to.
%
\begin{figure}[h]
  \begin{align}
    m &\dblcolon= \dots \mid [[join j x = m in m]] \mid [[jump j v]]
    \tag{join points, jumps}
  \end{align}
  \caption{Extended configurations}
  \Description[]{}
  \label{fig:join}
\end{figure}

Inspired by \citet{join},
who tackle the same issue with commuting conversion in System F with case expressions,
we too add explicit join point and jump constructs to CBPV in \cref{fig:join}.
They are accompanied by typing rules that restrict where join points may be used,
which we cover in \cref{sec:cbpv}.
In contrast to \opcit,
our jumps aren't \Acom{}s and may only appear in tail position,
as commuting conversion normalization discards contexts around jumps.
This simplifies both the evaluation semantics and our proofs
while also ruling out extraneous terms such as $[[(jump j v) w]]$.
% All in all, these restrictions ensure that join points are only ever used by local tail jumps.

Coming back to the commuting conversion of \cref{eq:let-join},
we use join points in place of the bound thunk,
jumping to them after binding the branches.
%
\usetagform{brack}
\begin{align}
  & \nonumber {\color{gray} [[let x ← (case v of inl y1 ⇒ m1 ; inr y2 ⇒ m2) in m]]} \\
  \Longrightarrow \ &[[join j x = m in _]] \\
  & \nonumber \phantom{\kw{join} \gap}
  [[case v of inl y1 ⇒ (let x ← m1 in jump j x) ; inr y2 ⇒ (let x ← m2 in jump j x)]]
\end{align}
\usetagform{default}

\iffalse
Unfortunately, even with join points,
the case-in-case optimizations highlighted in \opcit are not performed,
as the translation to CBPV has already obscured these opportunities.
Case-in-case optimizations deal with case expressions in lambda calculus
whose scrutinees are themselves case expressions,
where a commuting conversion followed by an inlining simplifies away an inner case analysis.
Consider the following example in lambda calculus with conditionals,
where the left-hand side would be optimized with join points to the right-hand side.
%
\usetagform{brack}
\begin{align}
  &[[:concrete: if (if v then false else w) then m_1 else m_2]] \nonumber \\
  \Longrightarrow \ &[[join j = m2 in (if v then jump j else (if w then m1 else jump j))]]
\end{align}
\usetagform{default}

If we translate the left-hand side for both CBN and CBV to CBPV,
the inner conditional is bound in an intermediate let expression,
making its case-in-case nature difficult to identify.%
\footnote{\Citet{ANF-dead} also highlights this difficulty in the context of MNF.}
Further sequentialization (and inlining) with join points prevents duplication of $[[m1]]$ and $[[m2]]$,
but contains an extra conditional inside the join point,
which is unnecessary in the branch where $[[v]]$ is $[[true]]$
and we know the next computation to execute must be $[[m2]]$.
%
\begin{align}
  & {\color{gray} [[:concrete: if (if v then false else w) then m_1 else m_2]]} \nonumber \\
  \Longrightarrow \ & [[let x ← (if v then return false else return w) in (if x then m1 else m2)]] \tag{\textit{CBPV}} \\
  \Longrightarrow \ & [[join j x = (if x then m1 else m2) in _]] \tag{\textit{commute, inline}} \\
  & \phantom{\kw{join} \gap} [[(if v then jump j false else jump j w)]] \nonumber
\end{align}

Even so, join points still successfully solve the problem of preventing
duplication of $[[m1]]$ and $[[m2]]$ without creating a new thunk.
\fi

\section{CBPV with Join Points} \label{sec:cbpv}

While \cref{sec:overview} presents the source (plain CBPV) and target (CCNF CBPV with join points)
languages with distinct syntactic forms,
in our mechanization (and thus our technical presentation here),
we use a single unified syntax and treat CC-normalization as a source-to-source translation,
showing \aposteriori in \cref{sec:ccnf} that the output of the translation satisfies \cref{fig:ccnf,fig:join}.
In \cref{sec:proof}, we reason about equivalence between a CBPV term and its translation,
which requires both sides of the equivalence to belong to the same syntactic category
for the equivalence to have the properties of an equivalence relation.

\begin{figure}[h]
  \begin{align}
    [[A]] &\dblcolon= [[⊤]] \mid [[A + A]] \mid [[U B]] \tag{value types} \\
    [[B]] &\dblcolon= [[A → B]] \mid [[B & B]] \mid [[F A]] \tag{computation types} \\
    [[v]], [[w]] &\dblcolon= [[x]] \mid [[()]] \mid [[inl v]] \mid [[inr v]] \mid [[{m}]] \tag{values} \\
    [[m]], [[n]] &\dblcolon= [[v!]] \mid [[λx. m]] \mid [[n v]] \mid [[⟨m, m⟩]] \mid [[fst n]] \mid [[snd n]] \mid [[return v]] \tag{computations} \\
    &\mid [[let x ← n in m]] \mid [[case v of inl x ⇒ m ; inr y ⇒ m]] \nonumber \\
    &\mid \highlight{$[[join j x = m in m]]$} \mid \highlight{$[[jump j v]]$} \nonumber
  \end{align}
  \caption{Syntax of value, computations, and their types}
  \Description[]{}
  \label{fig:syntax-full}
\end{figure}

\Cref{fig:syntax-full} lists the full syntax of values and computations,
along with value types and computation types.
The new syntactic forms not found in plain CBPV are highlighted in salmon.
For clarity, although the mechanization uses de Bruijn indexing and simultaneous substitutions,
we present the syntax here in nominal form,
with \fbox{$[[v{x ↦ w}]]$}, \fbox{$[[m{x ↦ w}]]$} denoting single (capture-avoiding) substitution
of $[[x]]$ for $[[w]]$ in $[[v]]$ and $[[m]]$, respectively.
The syntax and related definitions in the mechanization are also intrinsically well scoped
with respect to jump variables, which we omit here,
and freely use jump well-scopedness throughout the proofs.

\subsection{Evaluation semantics}

The purpose of join points is best explained by what they do;
their single-step evaluation rules, written \fbox{$[[m ⇝ m']]$},
are listed in \cref{fig:eval}, alongside rules for the usual CBPV constructs.
We write \fbox{$[[m ⇝* m']]$} for their reflexive, transitive closure.
The names of the rules involving join points and jumps not found in plain CBPV are highlighted in salmon.

\begin{figure}[h]
  \begin{align}
    [[{m}! &⇝ m]]                         \tag{\ottdrulename{E-force}} \\
    [[(λx. m) v &⇝ m{x ↦ v}]]             \tag{\ottdrulename{E-app}} \\
    [[fst ⟨m1, m2⟩ &⇝ m1]]                \tag{\ottdrulename{E-fst}} \\
    [[snd ⟨m1, m2⟩ &⇝ m2]]                \tag{\ottdrulename{E-snd}} \\
    [[let x ← return v in m &⇝ m{x ↦ v}]] \tag{\ottdrulename{E-ret}} \\
    [[case (inl v) of inl x ⇒ m1 ; inr y ⇒ m2 &⇝ m1{x ↦ v}]] \tag{\ottdrulename{E-left}} \\
    [[case (inr v) of inl x ⇒ m1 ; inr y ⇒ m2 &⇝ m2{y ↦ v}]] \tag{\ottdrulename{E-right}} \\
    [[join j x = m in jump j v &⇝ m{x ↦ v}]]   \tag{\highlight{\ottdrulename{E-jump}}} \\
    [[join j' x = m' in jump j v &⇝ jump j v]] \tag{\highlight{\ottdrulename{E-skip}}} \\
    [[join j' x = m' in tm &⇝ tm]]             \tag{\highlight{\ottdrulename{E-drop}}}
  \end{align}

  \begin{mathpar}
    \inferrule*[right=\ottdrulename{E-cong}]{[[m1 ⇝ m2]]}{[[ E[ m1 ] ⇝ E[ m2 ] ]]} \and
    \inferrule*[right=\highlight{\ottdrulename{E-join}}]{[[m1 ⇝ m2]]}{[[ join j x = m in m1 ⇝ join j x = m in m2 ]]}
  \end{mathpar}

  \begin{align}
    \textit{where} \quad
    \tag{evaluation contexts}   [[E]]  &\dblcolon= [[let x ← □ in m]] \mid [[□ v]] \mid [[fst □]] \mid [[snd □]] \\
    \tag{terminal computations} [[tm]] &\dblcolon= [[λx. m]] \mid [[return v]] \mid [[⟨m, m⟩]]
  \end{align}
  \caption{Evaluation rules for computations}
  \Description[]{}
  \label{fig:eval}
\end{figure}

The usual rules \rref*{E-force} through \rref*{E-right} say that
thunks get forced to their inner computations,
applied functions substitute in their arguments,
the first and second components of a computational pair can be projected out,
and case analysis on the left or right injections
reduce to the left or right branches, respectively.
\Rref{E-cong} states that evaluation may occur under evaluation contexts $[[E]]$,
which are elimination forms with holes in scrutinee position.

Join expressions evaluate similarly to let expressions,
where evaluation occurs under the context $[[join j x = m in □]]$ in \rref{E-join};
we state the rule separately instead of including this context in $[[E]]$
to distinguish the new rules from the old.
The inner evaluation is done when it reaches a jump or
a terminal computation $[[tm]]$, which are computation introduction forms.
If the body of the join expression is a jump to the bound join point,
then the value jumped with is substituted into the join point in \rref{E-jump}.
If it jumps to a join point further outward,
then this inner join point binding is discarded in \rref{E-skip}.
The binding is also discarded when a terminal is reached in \rref{E-drop}.

To illustrate, consider the following reduction sequence for three join points.
Intuitively, we jump to $[[j3]]$ which jumps to $[[j1]]$ which is $[[m1]]$,
so we ought to end up at $[[m1{x ↦ v}]]$;
the reductions that apply to get there are first to jump, then to skip, then to jump.
%
\begin{align}
  &[[join j1 x = m1 in join j2 x = m2 in join j3 x = jump j1 x in jump j3 v]] \nonumber \\
  &[[_ ⇝ join j1 x = m1 in join j2 x = m2 in jump j1 v]] \tag{\rref*{E-jump}} \\
  &[[_ ⇝ join j1 x = m1 in jump j1 v]] \tag{\rref*{E-skip}} \\
  &[[_ ⇝ m1{x ↦ v}]] \tag{\rref*{E-jump}}
\end{align}

This is in fact the \emph{only} sequence of reductions,
because evaluation is deterministic.
If we define normalization as evaluation to a terminal,%
\footnote{This is weak normalization, not strong normalization,
since subterms are not normalized,
but we won't consider strong normalization,
so we simply call it normalization.}
then normalization is deterministic as well,
and multi-step evaluations always evaluate to the same terminal.

\begin{lemma}[Determinism of evaluation]
  If $[[m ⇝ m1]]$ and $[[m ⇝ m2]]$,
  then $[[m1]] = [[m2]]$.
\end{lemma}

\begin{definition}[Normalization]
  $[[m]]$ normalizes to $[[tm]]$,
  written as \fbox{$[[m ⇓ tm]]$},
  if $[[m ⇝* tm]]$.
\end{definition}

\begin{corollary}[Determinism of normalization]
  If $[[m ⇓ tm1]]$ and $[[m ⇓ tm2]]$,
  then $[[tm1]] = [[tm2]]$.
\end{corollary}

\begin{corollary}[Merging] \label{lem:eval:merging}
  If $[[m ⇓ tm]]$ and $[[m ⇝* m']]$,
  then $[[m' ⇓ tm]]$.
\end{corollary}

\subsection{Typing rules}

By adding join points, we now have two different forms of bindings:
value variables $[[x]]$ bind values and have value types,
while jump variables $[[j]]$ bind join points,
which are computations of type $[[B]]$ that take some value argument of type $[[A]]$.
As the bindings have different types, we use two different typing contexts:
$[[G]] \dblcolon= [[•]] \mid [[G, x : A]]$ for value contexts, and
$[[D]] \dblcolon= [[•]] \mid [[D, j : A ↗ B]]$ for jump contexts.

\begin{figure}[h]
  \begin{mathpar}
    \fbox{$[[G ⊢ v : A]]$} \qquad \fbox{$[[G | D ⊢ m : B]]$} \hfill \\
    \inferrule[\ottdrulename{T-var}]
      {[[x : A ∈ G]]}
      %-------------%
      {[[G ⊢ x : A]]}
    \and
    \inferrule[\ottdrulename{T-unit}]{~}{[[G ⊢ () : ⊤]]}
    \and
    \inferrule[\ottdrulename{T-left}]
      {[[G ⊢ v : A1]]}
      %-----------------------%
      {[[G ⊢ inl v : A1 + A2]]}
    \and
    \inferrule[\ottdrulename{T-right}]
      {[[G ⊢ v : A2]]}
      %-----------------------%
      {[[G ⊢ inr v : A1 + A2]]}
    \and
    \inferrule[\ottdrulename{T-thunk}]
      {[[G | • ⊢ m : B]]}
      %---------------------%
      {[[G ⊢ {m} : U B]]}
    \and
    \inferrule[\ottdrulename{T-force}]
      {[[G ⊢ v : U B]]}
      %------------------%
      {[[G | D ⊢ v! : B]]}
    \and
    \inferrule[\ottdrulename{T-fun}]
      {[[G, x : A | • ⊢ m : B]]}
      %-------------------------%
      {[[G | D ⊢ λx. m : A → B]]}
    \and
    \inferrule[\ottdrulename{T-app}]
      {[[G | • ⊢ n : A → B]] \\
       [[G ⊢ v : A]]}
      %-------------------%
      {[[G | D ⊢ n v : B]]}
    \and
    \inferrule[\ottdrulename{T-ret}]
      {[[G ⊢ v : A]]}
      %--------------------------%
      {[[G | D ⊢ return v : F A]]}
    \and
    \inferrule[\ottdrulename{T-let}]
      {[[G | • ⊢ n : F A]] \\
       [[G, x : A | D ⊢ m : B]]}
      %------------------------------%
      {[[G | D ⊢ let x ← n in m : B]]}
    \and
    \inferrule[\ottdrulename{T-case}]
      {[[G ⊢ v : A1 + A2]] \\
       [[G, x : A1 | D ⊢ m1 : B]] \\
       [[G, y : A2 | D ⊢ m2 : B]]}
      %-------------------------------------------------%
      {[[G | D ⊢ case v of inl x ⇒ m1 ; inr y ⇒ m2 : B]]}
    \and
    \inferrule[\ottdrulename{T-pair}]
      {[[G | • ⊢ m1 : B1]] \\
       [[G | • ⊢ m2 : B2]]}
      %------------------------------%
      {[[G | D ⊢ ⟨m1, m2⟩ : B1 & B2]]}
    \and
    \inferrule[\ottdrulename{T-fst}]
      {[[G | • ⊢ n : B1 & B2]]}
      %----------------------%
      {[[G | D ⊢ fst n : B1]]}
    \and
    \inferrule[\ottdrulename{T-snd}]
      {[[G | • ⊢ n : B1 & B2]]}
      %----------------------%
      {[[G | D ⊢ snd n : B2]]}
    \and
    \inferrule[\highlight{\ottdrulename{T-join}}]
      {[[G, x : A | D ⊢ m1 : B]] \\
       [[G | D, j : A ↗ B ⊢ m2 : B]]}
      %-----------------------------------%
      {[[G | D ⊢ join j x = m1 in m2 : B]]}
    \and
    \inferrule[\highlight{\ottdrulename{T-jump}}]
      {[[j : A ↗ B ∈ D]] \\
       [[G ⊢ v : A]]}
      %------------------------%
      {[[G | D ⊢ jump j v : B]]}
  \end{mathpar}
  \caption{Typing rules for values and computations}
  \Description[]{}
  \label{fig:typing}
\end{figure}

Without the jump contexts, rules \rref*{T-var} through \rref*{T-snd}
are the usual typing rules for the values and computations of plain CBPV.
When we include jump contexts, they are either threaded through rules unchanged,
or they are explicitly empty in premises for a few reasons:

\begin{itemize}
  \item Join represent local blocks of code that are jumped to,
    and aren't closures that capture their environment.
    The computation inside of a thunk in \rref{T-thunk}
    needs to be closed with respect to jump variables
    to prevent jumps from escaping their scopes.
  \item Jumping from inside a constructor violates type safety.
    For example, if we try to take a projection of $[[⟨jump j v, m⟩]]$,
    but $[[j]]$ jumps to a join point that isn't a pair, the program will crash.
    Thus the premises of \rref{T-pair} must have empty jump contexts,
    as well as \rref{T-fun} by a similar argument.
  \item We restrict jumps to tail positions, in contrast to \citet{join},
    who allow jumps in scrutinee positions,
    and require type polymorphism to assign arbitrary types to jumps.
    Our one-pass transformation doesn't require the flexibility afforded
    by permitting computations such as $[[fst (jump j v)]]$.
    The benefit is that we don't need extra reduction steps that discard unneeded contexts,
    such as $[[fst (jump j v) ⇝ jump j v]]$,
    and that a well-typed jump has a known fixed type according to the jump context,
    which makes it much easier to define the logical relation in \cref{sec:proof:logrel}.
    To enforce this restriction,
    the scrutinee premises of \rref{T-app,T-let,T-fst,T-snd}
    require empty jump contexts,
    while the remaining (tail) premises of \rref{T-let,T-case} may contain jumps.
\end{itemize}

In \rref{T-join}, we extend $[[D]]$ with a join declaration when checking the body $[[m2]]$,
which may jump to the join point $[[m1]]$.
Both have the same type $[[B]]$ because jumping in tail position doesn't change the type,
and \rref{T-jump} indicates that jumping to the join point at $[[j]]$
with a value of type $[[A]]$ indeed has the same $[[B]]$.
We can then, for example, jump to a join point in only one branch of a case expression
so long as the join point has the same type as the other branch,
as in the following derivable judgement.
%
\begin{mathpar}
  \mprset{fraction={{-~} {~-~} {~-}}}
  \infer
    {[[G ⊢ v : A1 + A2]] \\
     [[G, x : A1 | • ⊢ m1 : B]] \\
     [[G, y : A2 | • ⊢ m2 : B]]}
    %------------------------------------------------------------------------%
    {[[G | • ⊢ join j x = m1 in case v of inl x ⇒ jump j x ; inr y ⇒ m2 : B]]}
\end{mathpar}

The important typing lemmas that we need are weakening lemmas for both value and jump contexts.
In the mechanization, they are proven by induction on the typing derivation via renaming lemmas;
for now, we ignore issues of renaming de Bruijn indices, as they are standard.

\begin{lemma}[Weakening (value contexts)] \label{lem:wk:val}
  Suppose $[[x]]$ is not free in $[[m]]$.
  If $[[G | D ⊢ m : B]]$, then $[[G, x : A | D ⊢ m : B]]$.
  Similarly, if $[[G, y : A' | D ⊢ m : B]]$, then $[[G, x : A, y : A' | D ⊢ m : B]]$.
\end{lemma}

\begin{lemma}[Weakening (jump contexts)] \label{lem:wk:jump}
  Suppose $[[j]]$ is not free in $[[m]]$.
  If $[[G | D ⊢ m : B]]$, then $[[G | D, j : A ↗ B' ⊢ m : B]]$.
\end{lemma}

\section{Commuting Conversion Normalization} \label{sec:ccnf}

In \cref{sec:overview}, we listed four examples of commuting conversions as \crefrange{eq:let-if}{eq:app-if}.
The general formulation commutes evaluation contexts with \emph{tail contexts},
listed in \cref{fig:tail}, which are elimination forms with holes where computations continue,
which in our case are the bodies of let expressions and the branches of case expressions.
We can think of computations with a tail as those that have a ``next step''.
As with evaluation contexts, we exclude join point expressions from tail contexts
because our source language is plain CBPV with no join points or jumps.

\begin{figure}[h]
  \begin{align}
    \tag{tail contexts} [[L]] &\dblcolon= [[let x ← n in □]] \mid [[case v of inl x ⇒ □ ; inr y ⇒ □]]
  \end{align}
  \caption{Tail contexts}
  \Description[]{}
  \label{fig:tail}
\end{figure}

The commuting conversions we consider can then be stated as $[[ E[ L[m] ] ]] \Rightarrow [[ L[ E[ m] ] ]]$.
Informally, they unnest code because they move evaluation contexts into the ``next step'',
and they expose inlining opportunities because these evaluation contexts are no longer
blocked by tail contexts that still have a ``first step'' to compute.
The shape of commuting conversions inform the shape of its normal form of computations,
reproduced in \cref{fig:ccnf-join}, where \Acom{}s $[[n]]$ are computations that don't contain tail contexts,
and CC-normal computations $[[m]]$ are all computations that don't contain tail contexts in scrutinee positions,
\ie where holes appear in evaluation contexts.
By inspection, CCNF is indeed normal with respect to commuting conversions because no $[[m]]$s appear in $[[n]]$s,
and therefore there must be no more commuting conversions to do.

\begin{figure}[h]
  \begin{align}
    [[v]] &\dblcolon= [[x]] \mid [[()]] \mid [[inl v]] \mid [[inr v]] \mid [[{m}]] \tag{values} \\
    [[n]] &\dblcolon= [[v!]] \mid [[λx. m]] \mid [[n v]] \mid [[return v]] \mid [[⟨m, m⟩]] \mid [[fst n]] \mid [[snd n]] \tag{\Acom} \\
    [[m]] &\dblcolon= n \mid [[let x ← n in m]] \mid [[case v of inl x ⇒ m1 ; inr y ⇒ m2]] \tag{computations} \\
          &\mid [[join j x = m1 in m2]] \mid [[jump j v]] \nonumber
  \end{align}
  \caption{Commuting conversion normal form with join points}
  \Description[]{}
  \label{fig:ccnf-join}
\end{figure}

To transform a plain CBPV program to one in CCNF,
we follow \citet{ANF} and define a compiler using a continuation $[[K]]$,
whose forms are given in \cref{fig:kont}.
As usual, it can be the empty continuation $[[□]]$,
or it can be the let continuation $[[let x ← □ in m]]$:
a let expression is compiled by first translating the let-bound expression,
then binding its result to $[[x]]$, and finally continuing on with a translated computation $[[m]]$.

\begin{figure}[h]
  \begin{align*}
    [[K]] &\dblcolon= [[□]] \mid [[let x ← □ in m]] \mid [[ K[k] ]] &
    [[k]] &\dblcolon= [[□ v]] \mid [[fst □]] \mid [[snd □]]
  \end{align*}
  \caption{Commuting conversion normalization continuations}
  \Description[]{}
  \label{fig:kont}
\end{figure}

However, we have three more continuation forms corresponding to each of the remaining three evaluation contexts:
application, first projection, and second projection.
They are needed because in contrast to ANF for the lambda calculus,
functions and computation pairs are \emph{computations} and not \emph{values},
so they can't be let-bound, and their elimination forms can take arbitrary \Acom{}s.
To see how they're used, we turn to the mutual definitions of the translation
of values \fbox{$[[⟦v⟧]]$} and computations \fbox{$[[⟦m⟧ K]]$} in \cref{fig:CC-norm},
delaying the translation of case expressions for the moment.

% As an example, $[[(fst ⟨λx. return x, return w⟩) v]]$ is also valid CCNF.
% During the translation of this term, as we'll see shortly,
% we need to handle the inner computation $[[⟨λx. return x, return w⟩]]$
% using the sequence of continuations $[[ (□[□ v])[ fst □ ] ]]$.
% As shorthand, we also write this sequence as $[[(fst □) v]]$,
% plugging whatever is in brackets into the $[[□]]$ of the preceding continuation.
% We define the plugging operation \fbox{$[[ K[m] ]]$} in the same manner,
% so that $[[ ((fst □) v)[⟨λx. return x, return w⟩] ]]$ yields the original term.

\begin{figure}[h]
  \begin{subfigure}{0.35\textwidth}
    \begin{align*}
      % \mathclap{\fbox{$[[⟦v⟧]] \coloneqq v$} \hfill} \\
      [[⟦ x ⟧]] &\coloneqq [[x]] \\
      [[⟦()⟧]] &\coloneqq [[()]] \\
      [[⟦inl v⟧]] &\coloneqq [[inl ⟦v⟧]] \\
      [[⟦inr v⟧]] &\coloneqq [[inr ⟦v⟧]] \\
      [[⟦{m}⟧]] &\coloneqq [[{⟦m⟧□}]]
    \end{align*}
    \vspace{-\baselineskip}
    \caption{Translation of values}
    \label{fig:CC-norm-val}
  \end{subfigure}
  %
  \begin{subfigure}{0.55\textwidth}
    \begin{align*}
      % \mathclap{\fbox{$[[⟦m⟧ K]] \coloneqq [[n]]$} \hfill} \\
      [[⟦v!⟧ K]] &\coloneqq [[ K[⟦v⟧!] ]] \\
      [[⟦return v⟧ K]] &\coloneqq [[ K[return ⟦v⟧] ]] \\
      [[⟦λx. m⟧ K]] &\coloneqq [[ K[λx. ⟦m⟧□] ]] \\
      [[⟦⟨m1, m2⟩⟧ K]] &\coloneqq [[ K[⟨⟦m1⟧□, ⟦m2⟧□⟩] ]] \\
      [[⟦m v⟧ K]] &\coloneqq [[ ⟦m⟧ (K[□ v]) ]] \\
      [[⟦fst m⟧ K]] &\coloneqq [[ ⟦m⟧ (K[fst □]) ]] \\
      [[⟦snd m⟧ K]] &\coloneqq [[ ⟦m⟧ (K[snd □]) ]] \\
      [[⟦let x ← m1 in m2⟧ K]] &\coloneqq [[⟦m1⟧ (let x ← □ in ⟦m2⟧K)]]
    \end{align*}
    \vspace{-\baselineskip}
    \caption{Translation of computations}
    \label{fig:CC-norm-com}
  \end{subfigure}

  \caption{Commuting conversion normalization translation (excluding $\kw{case}$)}
  \Description[]{}
  \label{fig:CC-norm}
\end{figure}

The translation of computations takes a continuation as a second argument,
representing the rest of the computation that expects the result of the translated computation.
The translation of values is directly recursive on the term,
with the translation of thunks being the thunk
of the translated computation using the empty continuation,
since there's no computation left to do inside the thunk.

To define the translation of computations,
we need a plugging operation \fbox{$[[ K[m] ]]$}
that replaces the hole $[[□]]$ in the continuation $[[K]]$ by the given computation $[[m]]$.
If $[[K]]$ is a sequence of $[[k]]$, the leftmost $[[k]]$ represents the outermost continuation,
so we plug from right to left.
For example, given the continuation $[[ ((let x ← □ in m)[□ v])[fst □] ]]$,
plugging in a \Acom{} $[[n]]$ yields the computation \mbox{$[[let x ← (fst n) v in m]]$}.

As the holes only appear in $[[n]]$ positions, to produce a CCNF,
we can only plug \Acom{}s into continuations.
This is the case for the translations of forcing thunks, returning values, functions, and computation pairs:
we translate their subterms---%
using empty continuations if needed, since we don't commute into introduction forms---%
and plug them directly into the continuation.
(\Cref{sec:related} discusses why we don't commute.)

In the translation of let expressions,
the evaluation order would first compute $[[m1]]$, then $[[m2]]$,
followed by whatever computation remains in $[[K]]$.
Therefore, we translate $[[m1]]$ using a let continuation
that represents binding the result of $[[m1]]$ to $[[x]]$ then running the translation of $[[m2]]$.
For function applications and pair projections,
we translate the subterm, which we expect to yield a function or a pair,
under the continuation extended with application or projection, respectively.

To see how the translation yields CCNFs,
we can look at how they act on the left-hand side
of the commuting conversions in \cref{eq:let-let,eq:app-let},
assuming that $[[n]]$ is already in CCNF,
and using the fact that $[[⟦n⟧ K]] = [[ K[n] ]]$.
%
\begin{align}
  &  [[⟦let x ← (let y ← n in return v) in m⟧ □]]             \tag{\cref{eq:let-let}} \\
  &= [[⟦let x ← n in return v⟧ (let y ← □ in ⟦m⟧ □)]]         \tag{\textit{let}} \\
  &= [[⟦n⟧ (let x ← □ in ⟦return v⟧ (let y ← □ in ⟦m⟧ □))]]   \tag{\textit{let}} \\
  &= [[ (let x ← □ in (let y ← □ in ⟦m⟧ □)[return ⟦v⟧])[n] ]] \tag{\textit{$[[n]]$, ret}} \\
  &= [[let x ← n in (let y ← return ⟦v⟧ in ⟦m⟧ □)]]           \tag{\textit{plug}} \\
  &  [[⟦(let x ← n in λy. m) v⟧ □]]                           \tag{\cref{eq:app-let}} \\
  &= [[⟦let x ← n in λy. m⟧ (□ ⟦v⟧)]]                         \tag{\textit{app}} \\
  &= [[⟦n⟧ (let x ← □ in ⟦λy. m⟧ (□ ⟦v⟧))]]                   \tag{\textit{let}} \\
  &= [[ (let x ← □ in (□ ⟦v⟧)[λy. ⟦m⟧ □])[n] ]]               \tag{\textit{$[[n]]$, fun}} \\
  &= [[let x ← n in (λy. ⟦m⟧ □) ⟦v⟧]]                         \tag{\textit{plug}}
\end{align}

\hfill

The na\"ive translation of conditional expressions duplicates the continuation:
$$[[⟦if v then m1 else m2⟧K]] = [[if ⟦v⟧ then ⟦m1⟧ K else ⟦m2⟧ K]].$$
If $[[K]]$ only contains application and projection continuations,
this is the desired translation.
From $[[fst (if v then ⟨m1, m2⟩ else n)]]$ we get $[[if v then fst ⟨m1, m2⟩ else fst n]]$,
whose true branch can then be inlined.
It doesn't make sense to jump to a join point that performs the projection,
since it would make the inlining opportunity harder to find,
and require unnecessarily thunking the computation pair to pass it to the join point.

However, if the continuation is $[[let x ← □ in m]]$,
duplicating it would duplicate the arbitrary computation $[[m]]$.
In this case, we move $[[m]]$ to a join point and replace it in the continuation by a jump.
From $[[let x ← (if v then return w else n) in m]]$ we get
$$[[join j x = m in (if v then (let x ← return w in jump j x) else (let x ← n in jump j x))]],$$
and the true branch can still inline the let binding.
\Cref{fig:CC-norm-case} generalizes this strategy to case expressions and nested continuations;
whether we construct a join point depends on whether the continuation ends in a let continuation.

\begin{figure}[h]
  \begin{multline*}
    [[⟦case v of inl x ⇒ m1 ; inr y ⇒ m2⟧ K]] \coloneqq \\
    \begin{cases}
      \: [[case ⟦v⟧ of inl y ⇒ ⟦m1⟧ K ; inr z ⇒ ⟦m2⟧ K]]
      &\textit{if} ~{}~ [[K]] \equiv [[□]] [k_1] \dots [k_i] \\
      \: [[join j x = m in _]] &\textit{if} ~{}~ [[K]] \equiv [[(let x ← □ in m)]][ [[k1]] ] \dots [ [[ki]] ] \\
      \: \quad [[case ⟦v⟧ of inl y ⇒ ⟦m1⟧ K' ; inr z ⇒ ⟦m2⟧ K']]
    \end{cases}
    \\
    \textit{where} ~{}~ [[K']] \coloneqq [[(let x ← □ in jump j x)]][ [[k1]] ] \dots [ [[ki]] ]
  \end{multline*}
  \caption{Translation of case expressions to CCNF with join points}
  \Description[]{}
  \label{fig:CC-norm-case}
\end{figure}

In the mechanization, this is a source-to-source translation
over CBPV terms (without join points) to CBPV terms (with join points).
We need to explicitly prove that the translation indeed produces terms in CCNF,
which holds by inspection, \ie mutual induction over the syntax.

\begin{lemma}[Plugging preserves CCNF] \leavevmode \\
  If the subterms of $[[K]]$ are in CCNF and $[[n]]$ is in CCNF,
  then $[[ K[n] ]]$ is in CCNF.
\end{lemma}

\begin{lemma}[CCNF preservation] \label{lem:ccnf-preservation} \leavevmode \\
  If the subterms of $[[K]]$ are in CCNF,
  then $[[⟦v⟧]]$ and $[[⟦m⟧ K]]$ are in CCNF.
\end{lemma}

\subsection{Commuting conversion preserves typing}

With \cref{lem:ccnf-preservation},
we know that the translation performs all the commuting conversions
in the directions we desire.
However, this alone doesn't guarantee that the translation preserves the \emph{meaning}
of the values and computations.
We say what we mean by meaning preservation in the next section,
whose proof uses typing preservation:
a translated term must behave like the same kind of term as the original term.

\begin{figure}[h]
  \begin{mathpar}
    \fbox{$[[G | D ⊢ K : B1 ⇒ B2]]$} \hfill \\
    \inferrule[\ottdrulename{K-let}]
      {[[G, x : A | D ⊢ m : B]]}
      %------------------------------------%
      {[[G | D ⊢ let x ← □ in m : F A ⇒ B]]}
    \and
    \inferrule[\ottdrulename{K-app}]
      {[[G ⊢ v : A]] \\
       [[G | D ⊢ K : B1 ⇒ B2]]}
      %----------------------------------%
      {[[G | D ⊢ K[□ v] : (A → B1) ⇒ B2]]}
    \and
    \inferrule[\ottdrulename{K-fst}]
      {[[G | D ⊢ K : B1 ⇒ B]]}
      %------------------------------------%
      {[[G | D ⊢ K[fst □] : (B1 & B2) ⇒ B]]}
    \quad
    \inferrule[\ottdrulename{K-snd}]
      {[[G | D ⊢ K : B1 ⇒ B]]}
      %------------------------------------%
      {[[G | D ⊢ K[snd □] : (B1 & B2) ⇒ B]]}
    \quad
    \inferrule[\ottdrulename{K-hole}]{~}
      {[[G | D ⊢ □ : B ⇒ B]]}
  \end{mathpar}
  \caption{Typing rules for continuations}
  \Description[]{}
  \label{fig:typing-K}
\end{figure}

To show type preservation, we first need a typing judgement for the continuations,
given in \cref{fig:typing-K}.
A continuation $[[K]]$ having type $[[B1 ⇒ B2]]$ means that
if the hole in $[[K]]$ represents a missing computation of type $[[B1]]$,
then $[[K]]$ with its hole plugged in would be a computation of type $[[B2]]$.
Consequently, we can show that plugging preserves typing,
which is used in the proof of type preservation.
We also need an inversion lemma for continuations that end in let continuations.

\begin{lemma}[Plugging preserves typing] \label{lem:preservation:plug} \leavevmode \\
  If $[[G | D ⊢ K : B1 ⇒ B2]]$ and $[[G | • ⊢ n : B1]]$,
  then $[[G | D ⊢ K[n] : B2]]$.
\end{lemma}

\begin{proof}
  By induction on the typing derivation of $[[K]]$,
  using \cref{lem:wk:jump} in the \rref*{K-hole} case.
\end{proof}

\begin{lemma}[Let continuation inversion] \label{lem:k-let:inv}
  If $[[K]] = [[(let x ← □ in m)]][ [[k1]] ] \dots [ [[ki]] ]$
  and $[[G | D ⊢ K : B1 ⇒ B2]]$,
  then there exists some $[[A]]$ such that
  $[[G | D, j : A ↗ B2 ⊢ K' : B1 ⇒ B2]]$
  and $[[G, x : A | D ⊢ m : B2]]$,
  where $[[K']] = [[(let x ← □ in jump j x)]][ [[k1]] ] \dots [ [[ki]] ]$.
\end{lemma}

\begin{proof}
  By induction on the typing derivation of $[[K]]$.
\end{proof}

\begin{lemma}[Type preservation]
  Suppose $[[v]]$ and $[[m]]$ are plain CBPV terms
  (and thus have no join points or jumps).
  If $[[G ⊢ v : A]]$, then $[[G ⊢ ⟦v⟧ : A]]$,
  and if $[[G | • ⊢ m : B1]]$ and $[[G | D ⊢ K : B1 ⇒ B2]]$,
  then $[[G | D ⊢ ⟦m⟧ K : B2]]$.
\end{lemma}

\begin{proof}
  By mutual induction on the typing derivations of $[[v]]$ and $[[m]]$,
  using \cref{lem:preservation:plug} in the \rref*{T-force,T-fun,T-pair} cases.
  In the \rref*{T-case} case,
  if $[[K]]$ is a let continuation,
  use \cref{lem:k-let:inv} and \rref{T-join} to construct the derivation.
\end{proof}

\section{Commuting Conversion Normalization Preserves Evaluation} \label{sec:proof}

Normalizing by all commuting conversions shouldn't affect the meaning of a program.
Formally, if a closed computation evaluates to a returned value,
then its translation runs to the same value.
Because we don't evaluate inside of thunks,
we consider only computations that return \emph{ground} values,
which are those of type $[[T]] \dblcolon= [[⊤]] \mid [[T + T]]$.
Otherwise, a thunk and its CCNF are values that don't evaluate any further,
but aren't necessarily syntactically equal.

\begin{theorem} \label{thm:ground-run}
  Given $[[m]]$ such that $[[• | • ⊢ m : F T]]$,
  if $[[m ⇓ return v]]$, then $[[⟦ m ⟧ □ ⇓ return v]]$.
\end{theorem}

We prove this property as a corollary of a logical equivalence between a computation and its translation.
This machinery is required because the simpler method using a simulation argument,
such as the following statement, unfortunately doesn't work.

\begin{fail}[Simulation]
  If $[[m ⇝ n]]$ then $[[⟦m⟧ □ ⇝* ⟦n⟧ □]]$.
\end{fail}
\begin{proof}[Counterexample]
  Suppose we have CCNF computations $[[n1]]$, $[[n2]]$ and configuration $[[m]]$.
  Consider the term $[[let x ← {let y ← n1 in n2}! in m]]$,
  which reduces to $[[let x ← (let y ← n1 in n2) in m]]$.
  The left-hand side translates to itself, since it's already in CCNF,
  while the right-hand side translates to $[[let y ← n1 in let x ← n2 in m]]$.
  By transitivity, it remains to show that
  $[[let x ← (let y ← n1 in n2) in m ⇝* let y ← n1 in let x ← n2 in m]]$,
  but there is no such reduction sequence since there is no reduction step that commutes let bindings.
\end{proof}

However, if we know that $[[n1]]$ in the counterexample reduces to some $[[return v]]$,
then we can deduce that the right side and its translation must reduce to
$[[let x ← n2{y ↦ v} in m]]$, which gives us an equivalence between the original term and its translation.
If our counterexample is well typed,
then a logical equivalence gives us precisely the required information that
subterms must reduce to canonical terms such as returned values.

To handle translations with arbitrary translation continuation $[[K]]$,
we generalize to proving that a translation $[[⟦m⟧ K]]$
must be equivalent to plugging the computation back in as $[[ K[m] ]]$.
The proof of \cref{thm:ground-run} then proceeds by:
\begin{enumerate}
  \item \label{item:logrel} Defining a standard logical equivalence over CBPV types;
  \item \label{item:semeq} Closing over term and join point contexts with a semantic equivalence;
  \item \label{item:fundamental} Proving the fundamental theorem this equivalence,
    namely that well-typed terms are semantically equivalent to themselves;
  \item \label{item:cc} Showing that well-typed commuting conversions are in the semantic equivalence;
  \item \label{item:kont} Defining semantic equivalence of continuations
    and proving its fundamental theorem;
  \item \label{item:plug} Showing that plugging continuations respects semantic equivalence; and finally,
  \item \label{item:end} Proving that given a well-typed computation and a well-typed continuation,
    plugging the continuation with the computation
    is equivalent to translating the computation using the continuation.
\end{enumerate}
The theorem then holds by instantiating with the empty continuation.
\Cref{sec:proof:logrel} covers \crefrange{item:logrel}{item:fundamental},
\cref{sec:proof:cc} covers \cref{item:cc},
\cref{sec:proof:plug} covers \crefrange{item:kont}{item:plug},
and \cref{sec:proof:end} covers \cref{item:end}.

\subsection{Logical equivalence and the fundamental theorem} \label{sec:proof:logrel}

\begin{figure}[h]
  \begin{align*}
    \mathclap{\fbox{$[[(v, w) ∈ A]]$} \quad \fbox{$[[(m, n) ∈ B]]$} \quad \fbox{$[[(m, n) ∈* B]]$}} \hfill \\
    [[((), ()) ∈ ⊤]] & \\
    [[(inl v, inl w) ∈ A1 + A2]]       &\textit{ iff } [[(v, w) ∈ A1]] \\
    [[(inr v, inr w) ∈ A1 + A2]]       &\textit{ iff } [[(v, w) ∈ A2]] \\
    [[({m}, {n}) ∈ U B]]               &\textit{ iff } [[(m, n) ∈* B]] \\
    [[(return v, return w) ∈ F A]]     &\textit{ iff } [[(v, w) ∈ A]] \\
    [[(λx. m, λx. n) ∈ A → B]]         &\textit{ iff } \forall [[v]], [[w]] \mathpunct{.} [[(v, w) ∈ A]] ⇒ [[(m{x ↦ v}, n{x ↦ w}) ∈* B]] \\
    [[(⟨m1, m2⟩, ⟨n1, n2⟩) ∈ B1 & B2]] &\textit{ iff } [[(m1, n1) ∈* B1]] \wedge [[(m2, n2) ∈* B2]] \\
    [[(m, n) ∈* B]]                    &\textit{ iff } \exists [[tm1]], [[tm2]] \mathpunct{.} ([[m ⇓ tm1]]) \wedge ([[n ⇓ tm2]]) \wedge [[(tm1, tm2) ∈ B]]
  \end{align*}
  \caption{Logical equivalence of terms over CBPV types}
  \Description[]{}
  \label{fig:logeq}
\end{figure}

Our logical relation relates two closed values or computations at a value or computation type, respectively.
We equivalently say that a pair of terms are in the interpretation of some type.
Following the presentation by \citet{CBPV-Coq},
we use an auxiliary interpretation $[[⟦B⟧*]]$ which relates computations
that normalize to terminals related at $[[B]]$.
It follows that if $[[(m, n) ∈ B]]$, then $[[(m, n) ∈* B]]$.

The interpretations, defined by mutual recursion over types, are otherwise standard:
unit values are related,
left and right injections are related when their values are related at the respective left or right type,
thunks are related when their computations are related,
functions are related when their bodies are related for all related arguments,
and pairs are related when their first and second components are respectively related.

The first property we need of logical equivalence is backward closure under evaluation of $[[⟦B⟧*]]$,
which holds by unfolding its definition and transitivity of evaluation.
A helpful corollary adds congruence under join points.

\begin{lemma}[Backward closure] \label{lem:logeq:bwds} \leavevmode \\
  If $[[m1 ⇝ m2]]$ and $[[n1 ⇝ n2]]$ and $[[(m2, n2) ∈* B]]$,
  then $[[(m1, n1) ∈* B]]$
\end{lemma}

\begin{lemma}[Backward closure under join points] \label{lem:logeq:bwds-joins}
  If $[[m1 ⇝ m2]]$ and $[[n1 ⇝ n2]]$ and $[[(m2, n2) ∈* B]]$,
  then $[[(join j x = m' in m1, join j x = n' in m2) ∈* B]]$,
  using \rref{E-join,E-drop}.
\end{lemma}

The second property is symmetry and transitivity,
making it a partial equivalence relation over all terms.
These lemmas are proven by induction over the type and unfolding definitions.

\begin{lemma}[Symmetry and transitivity (logical equivalence)] \label{lem:logeq:PER} \leavevmode
  \begin{itemize}
    \item If $[[(v, w) ∈ A]]$ then $[[(w, v) ∈ A]]$,
    \item If $[[(m, n) ∈ B]]$ then $[[(n, m) ∈ B]]$.
    \item If $[[(m, n) ∈* B]]$ then $[[(n, m) ∈* B]]$.
    \item If $[[(v1, v2) ∈ A]]$ and $[[(v2, v3) ∈ A]]$ then $[[(v1, v3) ∈ A]]$,
    \item If $[[(m1, m2) ∈ B]]$ and $[[(m2, m3) ∈ B]]$ then $[[(m1, m3) ∈ B]]$.
    \item If $[[(m1, m2) ∈* B]]$ and $[[(m2, m3) ∈* B]]$ then $[[(m1, m3) ∈* B]]$.
  \end{itemize}
\end{lemma}

\begin{figure}[h]
  \begin{align*}
    [[s]], [[t]] &\dblcolon= [[•]] \mid [[s, x ↦ v]] &
    [[p]], [[q]] &\dblcolon= [[•]] \mid [[p, j x = m]]
  \end{align*}
  \caption{Substitution maps and join stacks}
  \Description[]{}
  \label{fig:ctxt-subst}
\end{figure}

To work with equivalence of well-typed terms,
which may be open with respect to value and jump contexts,
we need close over value and jump variables
using substitution maps $[[s]]$ and join stacks $[[p]]$,
defined in \cref{fig:ctxt-subst}.
Applying a substitution map $[[v{s}]]$, $[[m{s}]]$
corresponds to simultaneous substitution (so order doesn't matter in $[[s]]$).
Given a join stack $[[p]] \equiv [[j1 x1 = m1]], \dots, [[ji xi = mi]]$,
the computation $[[joins p in m]]$ represents wrapping $[[m]]$ in join points outward in,
corresponding to the term $[[join j1 x1 = m1 in _]] \dots [[join ji xi = mi in m]]$
(so order \emph{does} matter in $[[p]]$).

\begin{figure}
  \begin{mathpar}
    \fbox{$[[(s, t) ∈ G]]$} \quad \fbox{$[[(p, q) ∈ D]]$} \hfill \\
    % [[(s, t) ∈ G]] \textit{ iff } \forall [[x : A ∈ G]] \mathpunct{.} [[(s(x), t(x)) ∈ A]] \\
    % \mprset{fraction={{-~} {~-~} {~-}}}
    \inferrule[\ottdrulename{S-nil}]{~}{(•, •) ∈ •}
    \and
    \inferrule[\ottdrulename{S-cons}]
      {[[(s, t) ∈ G]] \and [[(v, w) ∈ A]]}
      %----------------------------------------%
      {[[((s, x ↦ v), (t, x ↦ w)) ∈ G, x : A]]}
    \and
    \inferrule[\ottdrulename{J-nil}]{~}{(•, •) ∈ •}
    \and
    \inferrule[\ottdrulename{J-cons}]
      {[[(p, q) ∈ D]] \and \forall [[v]], [[w]] \mathpunct{.} [[(v, w) ∈ A]] ⇒ [[(joins p in m{x ↦ v}, joins q in n{x ↦ w}) ∈ B]]}
      %-----------------------------------------------%
      {[[((p, j x = m), (q, j x = n)) ∈ D, j : A ↗ B]]}
  \end{mathpar}
  \caption{Logical equivalence of substitution maps and join stacks}
  \Description[]{}
  \label{fig:logeq-subst}
\end{figure}

We extend logical equivalence to substitution maps and join stacks,
relating two of them at a particular context.
\Cref{fig:logeq-subst} gives inductive rules for these logical equivalences.
\Rref{S-nil,S-cons} are equivalent to stating that
$[[(s, t) ∈ G]]$ holds when for all $[[x : A ∈ G]]$, $[[(s(x), t(x)) ∈ A]]$ holds.
\Rref{S-cons} states that we can extend a pair of related join stacks $[[p]]$ and $[[q]]$
by join points $[[m]]$ and $[[n]]$ of type $[[A ↗ B]]$
when the join points themselves are related at $[[B]]$ given arguments related at $[[A]]$
\emph{and} closed over by $[[p]]$ and $[[q]]$.
They need to be closed over since $[[m]]$ and $[[n]]$ themselves may jump to earlier join points.
We can show that substitution maps and join stacks are partial equivalence relations
by induction on their derivations.

\begin{lemma}[Symmetry and transitivity (logical equivalence at contexts)] \label{lem:logeq-subst:PER} \leavevmode
  \begin{itemize}
    \item If $[[(s, t) ∈ G]]$ then $[[(t, s) ∈ G]]$.
    \item If $[[(p, q) ∈ D]]$ then $[[(p, q) ∈ D]]$.
    \item If $[[(s1, s2) ∈ G]]$ and $[[(s2, s3) ∈ G]]$ then $[[(s1, s3) ∈ G]]$.
    \item If $[[(p1, p2) ∈ D]]$ and $[[(p2, p3) ∈ D]]$ then $[[(p1, p3) ∈ D]]$.
  \end{itemize}
\end{lemma}

Using substitution maps and join stacks, we can define semantic equivalence of open terms.
They are also partial equivalence relations,
which follows from \cref{lem:logeq:PER,lem:logeq-subst:PER}.

\begin{definition}[Semantic equivalence of values]
  Values $[[v]]$ and $[[w]]$ are semantically equivalent under context $[[G]]$ at type $[[A]]$,
  written \fbox{$[[G ⊧ v ~ w : A]]$},
  when for all substitution maps $[[(s, t) ∈ G]]$,
  $[[(v{s}, w{t}) ∈ A]]$ holds.
\end{definition}

\begin{definition}[Semantic equivalence of computations]
  Computations $[[m]]$ and $[[n]]$ are semantically equivalent under contexts $[[G]]$ and $[[D]]$ at type $[[B]]$,
  written \fbox{$[[G | D ⊧ m ~ n : B]]$},
  when for all substitution maps $[[(s, t) ∈ G]]$ and join stacks $[[(p, q) ∈ D]]$,
  $[[(joins p in m{s}, joins q in n{t}) ∈ B]]$ holds.
\end{definition}

\begin{lemma}[Symmetry and transitivity (semantic equivalence)] \leavevmode
  \begin{itemize}
    \item If $[[G ⊧ v ~ w : A]]$ then $[[G ⊧ w ~ v : A]]$.
    \item If $[[G | D ⊧ m ~ n : B]]$ then $[[G | D ⊧ n ~ m : B]]$.
    \item If $[[G ⊧ v1 ~ v2 : A]]$ and $[[G ⊧ v2 ~ v3 : A]]$ then $[[G ⊧ v1 ~ v3 : A]]$.
    \item If $[[G | D ⊧ m1 ~ m2 : B]]$ and $[[G | D ⊧ m2 ~ m3 : B]]$ then $[[G | D ⊧ m1 ~ m3 : B]]$.
  \end{itemize}
\end{lemma}

Finally, we show the fundamental theorem of semantic equivalence:
well-typed terms are semantically equivalent to themselves.

\begin{theorem}[Fundamental theorem of semantic equivalence] \label{thm:semeq}
  If $[[G ⊢ v : A]]$ then $[[G ⊧ v ~ v : A]]$,
  and if $[[G | D ⊢ m : B]]$ then $[[G | D ⊧ m ~ m : B]]$.
\end{theorem}

\begin{proof}
  \TODO
\end{proof}

\subsection{Semantic equivalence of commuting conversions} \label{sec:proof:cc}

As the translation normalizes with respect to all commuting conversions,
to show that semantic equivalence of the translation,
naturally we need to show semantic equivalence of the commuting conversions.
Recall that with four evaluation contexts and two tail contexts,
there are eight total commuting conversions to handle,
each assuming well-typedness of various subterms.
We present them as derivable rules in \cref{fig:semeq:cc} for legibility.

\begin{figure}[h]
  \begin{mathpar}
    \mprset{fraction={{-~} {~-~} {~-}}}
    \infer*[Right=\ottdrulename{let-let}]
      {[[G | • ⊢ let x ← n in m : F A]] \\
       [[G, y : A | D ⊢ m' : B]]}
      {[[G | D ⊧ let y ← (let x ← n in m) in m' ~ let x ← n in let y ← m in m' : B]]}
    \and
    \infer*[Right=\ottdrulename{let-app}]
      {[[G | • ⊢ let x ← n in m : A → B]] \\
       [[G ⊢ v : A]]}
      {[[G | D ⊧ (let x ← n in m) v ~ let x ← n in (m v) : B]]}
    \and
    \infer*[Right=\ottdrulename{let-fst}]
      {[[G | • ⊢ let x ← n in m : B1 & B2]]}
      {[[G | D ⊧ fst (let x ← n in m) ~ let x ← n in (fst m) : B1]]}
    \and
    \infer*[Right=\ottdrulename{let-snd}]
      {[[G | • ⊢ let x ← n in m : B1 & B2]]}
      {[[G | D ⊧ snd (let x ← n in m) ~ let x ← n in (snd m) : B2]]}
    \and
    \infer*[Right=\ottdrulename{case-let}]
      {[[G | • ⊢ case v of inl x ⇒ m1; inr y ⇒ m2 : F A]] \\
       [[G, z : A | D ⊢ m : B]]}
      {[[G | D ⊧ let z ← (case v of inl x ⇒ m1; inr y ⇒ m2) in m \\ ~ case v of inl x ⇒ (let z ← m1 in m); inr y ⇒ (let z ← m2 in m) : B]]}
    \and
    \infer*[Right=\ottdrulename{case-app}]
      {[[G | • ⊢ case v of inl x ⇒ m1; inr y ⇒ m2 : A → B]] \\
       [[G ⊢ w : A]]}
      {[[G | D ⊧ (case v of inl x ⇒ m1; inr y ⇒ m2) w \\ ~ case v of inl x ⇒ (m1 w); inr y ⇒ (m2 w) : B]]}
    \and
    \infer*[Right=\ottdrulename{case-fst}]
      {[[G | • ⊢ case v of inl x ⇒ m1; inr y ⇒ m2 : B1 & B2]]}
      {[[G | D ⊧ fst (case v of inl x ⇒ m1; inr y ⇒ m2) \\ ~ case v of inl x ⇒ (fst m1); inr y ⇒ (fst m2) : B1]]}
    \and
    \infer*[Right=\ottdrulename{case-snd}]
      {[[G | • ⊢ case v of inl x ⇒ m1; inr y ⇒ m2 : B1 & B2]]}
      {[[G | D ⊧ snd (case v of inl x ⇒ m1; inr y ⇒ m2) \\ ~ case v of inl x ⇒ (snd m1); inr y ⇒ (snd m2) : B2]]}
  \end{mathpar}
  \caption{Semantic equivalence of commuting conversions}
  \Description[]{}
  \label{fig:semeq:cc}
\end{figure}

\iffalse
\begin{lemma}[Semantic equivalence of commuting conversions] \label{lem:semeq:cc} \leavevmode \\
  \begin{itemize}
    \item If $[[G | • ⊢ let x ← n in m : F A]]$ and $[[G, y : A | D ⊢ m' : B]]$, then
      $[[G | D ⊧ let y ← (let x ← n in m) in m' ~ let x ← n in let y ← m in m' : B]]$.
    \item If $[[G | • ⊢ let x ← n in m : A → B]]$ and $[[G ⊢ v : A]]$, then
      $[[G | D ⊧ (let x ← n in m) v ~ let x ← n in (m v) : B]]$.
    \item If $[[G | • ⊢ let x ← n in m : B1 & B2]]$, then
      $[[G | D ⊧ fst (let x ← n in m) ~ let x ← n in (fst m) : B1]]$.
    \item If $[[G | • ⊢ let x ← n in m : B1 & B2]]$, then
      $[[G | D ⊧ snd (let x ← n in m) ~ let x ← n in (snd m) : B2]]$.
    \item If $[[G | • ⊢ case v of inl x ⇒ m1; inr y ⇒ m2 : F A]]$ and $[[G, z : A | D ⊢ m : B]]$, then
      $[[G | D ⊧ let z ← (case v of inl x ⇒ m1; inr y ⇒ m2) in m ~ case v of inl x ⇒ (let z ← m1 in m); inr y ⇒ (let z ← m2 in m) : B]]$.
    \item If $[[G | • ⊢ case v of inl x ⇒ m1; inr y ⇒ m2 : A → B]]$ and $[[G ⊢ w : A]]$, then
      $[[G | D ⊧ (case v of inl x ⇒ m1; inr y ⇒ m2) w ~ case v of inl x ⇒ (m1 w); inr y ⇒ (m2 w) : B]]$.
    \item If $[[G | • ⊢ case v of inl x ⇒ m1; inr y ⇒ m2 : B1 & B2]]$, then
      $[[G | D ⊧ fst (case v of inl x ⇒ m1; inr y ⇒ m2) ~ case v of inl x ⇒ (fst m1); inr y ⇒ (fst m2) : B1]]$.
    \item If $[[G | • ⊢ case v of inl x ⇒ m1; inr y ⇒ m2 : B1 & B2]]$, then
      $[[G | D ⊧ snd (case v of inl x ⇒ m1; inr y ⇒ m2) ~ case v of inl x ⇒ (snd m1); inr y ⇒ (snd m2) : B2]]$.
  \end{itemize}
\end{lemma}
\fi

The general strategy to proving these is by using the \cref{thm:semeq} on the typing derivations,
extracting a logical equivalence for the desired type,
applying \cref{lem:logeq:bwds} or \cref{lem:logeq:bwds-joins},
then using the evaluation rules and \cref{lem:eval:merging}
to find the correct evaluation sequence from the left and right sides of the lemma statement
to the respective sides of the extracted logical equivalence.
We step through only the proof of \rref*{let-app} as a representative case.

\begin{proof}
  \TODO
\end{proof}

\TODO: \rref*{join-join}

\subsection{Semantic equivalence of plugging continuations} \label{sec:proof:plug}

Just as the \nameref{thm:semeq} requires related substitution maps and related join stacks,
for the final proof in the next section, we need a notion of related continuations.
Rather than first defining a logical equivalence over continuations,
we directly define semantic equivalence in a way that incorporates plugging.
Here, $[[K{s}]]$ denotes applying the substitution map $[[s]]$
to all value and computation subterms of $[[K]]$.

\begin{definition}[Semantic equivalence of continuations]
  Continuations $[[K1]]$ and $[[K2]]$ are semantically equivalent
  under contexts $[[G]]$ and $[[D]]$ at type $[[B1 ⇒ B2]]$,
  written \fbox{$[[G | D ⊧ K1 ~ K2 : B1 ⇒ B2]]$},
  when for all substitution maps $[[(s, t) ∈ G]]$, join stacks $[[(p, q) ∈ D]]$,
  and computations $[[(n1, n2) ∈* B1]]$,
  $[[(joins p in K{s}[n1], joins q in K{t}[n2]) ∈ B2]]$ holds.
\end{definition}

A fundamental theorem for semantic equivalence of continuations holds as well,
which relies on the corresponding theorem for terms,
as well as congruence of evaluating under plugging.

\begin{lemma}[\textsc{E-plug}] \label{lem:E-plug}
  If $[[n1 ⇝* n2]]$, then $[[ K[n1] ⇝* K[n2] ]]$,
  by induction on the structure of $[[K]]$.
\end{lemma}

\begin{theorem}[Fundamental theorem of semantic equivalence of continuations] \label{thm:semeq:K} \leavevmode \\
  If $[[G | D ⊢ K : B1 ⇒ B2]]$ then $[[G | D ⊧ K ~ K : B1 ⇒ B2]]$.
\end{theorem}

\begin{proof}
  By induction on the typing derivation of $[[K]]$.
  In the \rref*{K-app} case, use \cref{thm:semeq} on the value argument,
  and in the \rref*{K-let} case, use \cref{thm:semeq} on the computation body.
  In the \rref*{K-app,K-fst,K-snd} cases,
  use \nameref{lem:E-plug} to reduce under the rest of the continuation.
\end{proof}

Plugging semantically equivalent computations into both
the same continuation and equivalent continuations
then yield equivalent computations
as long as substitution commutes with plugging.

\begin{lemma}[Substitution commutes with plugging] \label{lem:subst-plug} \leavevmode \\
  $[[(K[n]){s}]] = [[(K{s})[n{s}] ]]$,
  by induction on the structure of $[[K]]$.
\end{lemma}

\begin{lemma}[Semantic equivalence of plugging] \label{lem:semeq:plug}
  If $[[G | D ⊧ K1 ~ K2 : B1 ⇒ B2]]$ and $[[G | • ⊧ n1 ~ n2 : B1]]$,
  then $[[G | D ⊧ K1[n1] ~ K2[n2] : B2]]$,
  trivially after rewriting by \cref{lem:subst-plug}.
\end{lemma}

\begin{corollary} \label{cor:semeq:plug}
  If $[[G | D ⊢ K : B1 ⇒ B2]]$ and $[[G | • ⊧ n1 ~ n2 : B1]]$,
  then $[[G | D ⊧ K[n1] ~ K[n2] : B2]]$,
  using \cref{lem:semeq:plug} and \cref{thm:semeq:K}.
\end{corollary}

\subsection{Commuting conversion normalization is semantically equivalent to plugging} \label{sec:proof:end}

\section{Related Work and Discussion} \label{sec:related}

\Citet{CBPV-Coq} mechanize in Rocq a vast amount of metatheory for call-by-push-value,
including normalization and observational equivalence.
Our work builds on their design of logical equivalence between terms.
They show a number of commuting conversions as semantic equivalences,
namely commuting let-bound let expressions (Lemma 8.4, Equation 5)
and function application of let expressions (Lemma 8.6, Equation 1),
but are not comprehensive in listing all possible commuting conversions systematically
as we have done in (\TODO: eventually).

Conversely, while we consider commuting conversions to only involve elimination forms,
\opcit also consider equivalences involving the function introduction form
(Lemma 8.5, Equation 3; Lemma 8.6, Equation 2),
listed below with two more for the pair introduction form.
%
\begin{align*}
  [[let x ← n in λy. m]]
    &\Longleftrightarrow [[λy. let x ← n in m]] \\
  [[case v of inl y ⇒ λx. m1 ; inr z ⇒ λx. m2]]
    &\Longleftrightarrow [[λx. case v of inl y ⇒ m1 ; inr z ⇒ m2]] \\
  [[let x ← n in ⟨m1, m2⟩]]
    &\Longleftrightarrow [[⟨let x ← n in m1, let x ← n in m2⟩]] \\
  [[case v of inl y ⇒ ⟨m1, m2⟩ ; inr z ⇒ ⟨m3, m4⟩]]
    &\Longleftrightarrow \\
    \mathclap{[[⟨case v of inl y ⇒ m1 ; inr z ⇒ m3, case v of inl y ⇒ m2 ; inr z ⇒ m4⟩]]}
\end{align*}

We don't consider these because it's unclear which direction to pick and what benefits they provide.
Going left to right, the case equations only apply when the branches are both exactly an introduction form,
and the pair equations must duplicate code.
Going right to left, the pair equations only apply when the projections both eliminate exactly the same thing.
Either way, they are sensitive to slight syntactic variation, \eg permuted let bindings.
Furthermore, neither transformation appears to reveal any more optimization opportunities.
% Taking the first equation as example,
% transforming left to right exposes the same function reduction opportunity
% as would pushing an application into the let expression,
% while transforming right to left destroys it and creates more work for us.

Our join points and their typing judgements with a separate join point typing context
are inspired by \citet{join}, who add join points and jumps to System F,
and implement their system in the Haskell compiler GHC.
We simplify the addition by only allowing jumps in tail position,
while they permit jumps in evaluation contexts.
They perform their optimizations equation by equation,
which yields intermediate steps that require this permissiveness,
such as the following.
%
\begin{align*}
  [[(join j x = m in jump j v) w]] &\Longrightarrow [[join j x = m w in (jump j v) w]] \\
                                   &\Longrightarrow [[join j x = m w in jump j v]]
\end{align*}
Because we present instead a single-pass algorithm,
we never produce such intermediate steps that need to be typeable,
so we are able to eliminate them from our syntax outright.
Doing so also simplifies typing:
their jumps need to be typeable with arbitrary types,
which they implement using type polymorphism,
while our jumps are always in tail position
and fixed to the return type of the join point they jump to.

Instead of specialized join and jump constructs,
\citet{wowo} use control operators that bind second-class continuations,
along with a type system that distinguishes between first- and second-class values.
Their system allows for both direct style and continuation-passing style optimizations;
this is not directly applicable to CBPV, which is already in direct style
by virtue of binding intermediate computations.
However, the corresponding dual of CBPV is stack-passing style \citep{compiling},
and it would be interesting to see whether a similar technique could be applied using second-class stacks.

\TODO: compare to \url{https://github.com/zydeco-lang/zydeco}

\section{Conclusion and Future Work} \label{sec:future}

\begin{itemize}
  \item \TODO: compiling to assembly \citep{CBPV-STAL} with and without CC-normalization optimizations
  \item \TODO: measuring stack size using cost model \citep{cost}
  \item \TODO: actual effects --- effects produced from evaluation are preserved and not reordered, tick + tock
  \item \TODO: real-life evaluation (for PLDI?)
\end{itemize}

\bibliography{main}

\end{document}
